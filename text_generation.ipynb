{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation\n",
    "\n",
    "The goal of this project is to demonstrate text generation using LSTM neural networks.\n",
    "Our database contains numerous movie plots taken from Wikipedia, so we will generate something similiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import LSTM, Dense, Dropout, Embedding\n",
    "from keras.models import Sequential\n",
    "import random \n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import os, multiprocessing\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computer characteristics: \n",
      "RAM: 7.653069 GB\n",
      "CORES: 4\n"
     ]
    }
   ],
   "source": [
    "# Computer characteristics\n",
    "\n",
    "mem_bytes = os.sysconf('SC_PAGE_SIZE') * os.sysconf('SC_PHYS_PAGES')\n",
    "mem_gib = mem_bytes / (1024.**3)\n",
    "print(\"Computer characteristics: \")\n",
    "print(\"RAM: %f GB\" % mem_gib)\n",
    "print(\"CORES: %d\" % multiprocessing.cpu_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max movie plot len:  6752\n",
      "Min movie plot len:  2\n",
      "Min movie plot len:  300\n",
      "Max movie plot len:  500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Set in the 1920s during China\\'s Warlord Era, The Wooden Man\\'s Bride is a romance film between two unlikely people from different classes. A young woman, known only as \"Young Mistress\" (Wang Lan), is on her way to meet her future husband for the first time when her procession is attacked by sword-wielding bandits in the desert. A servant, Kui (Chang Shih) recklessly takes off after the bandits who have kidnapped the young mistress and taken her back to their lair. When he arrives, Kui impresses the chief of the bandits (Kao Mingjun), who allows Kui to take the Young Mistress back.\\r\\nMeanwhile, word has reached the Young Mistress\\'s fiancé of her capture. Preparing to engage in a thrilling rescue, the hapless young man accidentally sets off an explosion, killing him. When Young Mistress finally arrives, she faces her fiancé\\'s imperious mother (Wang Yumei), who forces the young woman to undergo arcane tests of purity to determine whether she is worthy to marry the (now dead) bridegroom. When Young Mistress passes these tests, she is forced to marry the titular \"wooden man\", a wood-carved statue of her late fiancé.\\r\\nForced into a life she does not want, Young Mistress tries and fails to escape from her new home. She finds solace in her growing friendship with her one-time hero, Kui, a friendship that soon blossoms into an illicit love affair. When the affair is discovered, Kui is banished from the mill, and Madame Liu has the Young Mistress\\'s legs broken to prevent escape. Kui, however, is determined to rescue his love once more. He returns to the bandit\\'s lair to find the chief dead, and becomes the bandits\\' new leader.\\r\\nA year after he leaves, Kui returns with the bandits to claim Young Mistress. He allows Madame Liu to commit suicide by hanging herself, then burns the house down, taking Young Mistress away with him.'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"movie_plots.csv\")\n",
    "\n",
    "movie_plots = data['Plot']\n",
    "\n",
    "def count_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "print(\"Max movie plot len: \", movie_plots.map(count_words).max())\n",
    "print(\"Min movie plot len: \", movie_plots.map(count_words).min())\n",
    "\n",
    "\n",
    "# zadrzavamo samo opise sa vise od 300 reci\n",
    "movie_plots = movie_plots[movie_plots.map(count_words) > 299]\n",
    "print(\"Min movie plot len: \", movie_plots.map(count_words).min())\n",
    "# i zadrzavamo sve koji imaju manje od 500 reci\n",
    "movie_plots = movie_plots[movie_plots.map(count_words) < 501]\n",
    "print(\"Max movie plot len: \", movie_plots.map(count_words).max())\n",
    "\n",
    "all_plots = list(movie_plots.values)\n",
    "# setting a seed so we get the same result every time\n",
    "random.seed(64)\n",
    "sample = random.sample(all_plots, 100)\n",
    "sample[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize words\n",
    "\n",
    "Tokenization is turning unique words into unique integers. This step is necessary for preparing data for embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 820 2992    5    1   51    6    1 5646 5647    6    9  191   21   24\n",
      "   64    1 2822  257   16    1  437 2993  109  820    9   67    3    1\n",
      "  191  752   28 5648   17  974 5649    3 1194 5650 5651  664 2994 2995\n",
      "   51 1526 5652  120    2    1  191   23    1 1038  738    2  138 1527\n",
      " 5653 1422    1 5654   96    6  974    3    4  125    6 2994 1066   16\n",
      "   15    2  169   18    8   81  236    2  169 1528  102 1195 5655    4\n",
      " 1092 5656  125 1194  185    2    1  159   32    8  920   12    1 2815\n",
      "  425   46 1528   53   66 5657    2    1 1529 2907 5658 5659 5660    8\n",
      " 1319    1  425   20 1317  376   71    1 2967  159 1528   53  171    2\n",
      "   73   30   44   23    1 1529 1194 5661    1  159  974    5 2990   17\n",
      " 1196 1066   18 2531    1 1529  840   25  425    3  160    4 1243   11\n",
      " 1194 1196  159    5 5662   11 1527  228    9 1475  174   30   95  805\n",
      " 1475   12 1195    5  801   11 1196  498 1528   53 2050  315   26    8\n",
      "  920   12    1 1529   43  611   25  425  820    3 1195  725    2    1\n",
      "  190 2996 1196  133   14  266 1196  133 1195    5 2456  634   16 5663\n",
      "   71    9 2997 1622 1526 1876   10  820    5 5664   17 1194    7    1\n",
      "  331    6    4   72  379    3    5 1792 1192    2  118    8  558   23\n",
      "  868    3 5665  621 1527   21    5   60  206    2   49   85    2 1526\n",
      "   11    1  130    6    4  484 2998 1422 5666 2999    1 5667  128    6\n",
      "  974  820  120   33    6  545   19 5668    8    5   40 5669   17    9\n",
      "   53    7  350   34  258 5670    4  408 2995   19    1 5671   16   29\n",
      " 2036  700    2  887  974    4]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "max_words = 10000\n",
    "tokenizer = Tokenizer(num_words = max_words)\n",
    "tokenizer.fit_on_texts(sample)\n",
    "\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(sample)\n",
    "sequences_len = 300\n",
    "sequences = pad_sequences(sequences, maxlen = sequences_len, truncating = 'post')\n",
    "\n",
    "\n",
    "sequences.shape\n",
    "print(sequences[64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus size:  30000\n",
      "Vocabulary size:  7042\n"
     ]
    }
   ],
   "source": [
    "# making a single list of tokens so we can apply sliding windows\n",
    "\n",
    "text = [item for sublist in sequences for item in sublist]\n",
    "print(\"Corpus size: \", len(text))\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "print(\"Vocabulary size: \", vocab_size+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse dictionary so we can decode tokenized sequences back to words\n",
    "\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data for input and output values\n",
    "\n",
    "Input sequence has the size of 20 words, and output is the next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29980\n"
     ]
    }
   ],
   "source": [
    "seq_len = 20\n",
    "\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(len(text)-seq_len):\n",
    "    seq_in = text[i:i+seq_len]\n",
    "    seq_out = text[i+seq_len]\n",
    "    dataX.append(seq_in)\n",
    "    dataY.append(seq_out)\n",
    "    \n",
    "    \n",
    "lenX = len(dataX)\n",
    "print(lenX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "dataX = np.asarray(dataX)\n",
    "dataY = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23984, 20)\n",
      "(5996, 20)\n",
      "(23984, 7018)\n",
      "(5996, 7018)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "trainX, testX, trainy, testy = train_test_split(dataX, dataY, test_size = 0.2)\n",
    "print(trainX.shape)\n",
    "print(testX.shape)\n",
    "print(trainy.shape)\n",
    "print(testy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 20, 32)            225344    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 20, 100)           53200     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 20, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7018)              708818    \n",
      "=================================================================\n",
      "Total params: 1,067,762\n",
      "Trainable params: 1,067,762\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import LSTM, Dense, Embedding, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size+1, 32, input_length = trainX.shape[1]))\n",
    "model.add(LSTM(100,  return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(trainy.shape[1], activation = 'softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "\n",
    "filepath = \"./weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor = 'loss', \n",
    "                             verbose = 1, save_best_only = True, mode = 'min')\n",
    "callbacks = [checkpoint]\n",
    "\n",
    "hist = model.fit(trainX, trainy, epochs = 20, batch_size = 128, \n",
    "                 verbose = 1, callbacks = callbacks, validation_split = 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"20e100n.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "3996/3996 [==============================] - 2s 439us/step\n",
      "Loss: 5.11\n",
      "Accuracy: 25.98 %\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on test set (accuracy and error)\n",
    "\n",
    "print(model.metrics_names)\n",
    "results = model.evaluate(testX, testy, batch_size = 128)\n",
    "print('Loss: %.2f'% results[0])\n",
    "print('Accuracy: %.2f'%(results[1]*100), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot za trening skup (preciznost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nplt.plot(hist.history[\"acc\"])\\nplt.plot(hist.history[\"val_acc\"])\\nplt.title(\\'Model accuracy: \\')\\nplt.legend([\\'Train\\', \\'Validation\\'], loc=\\'upper right\\')\\nplt.ylabel(\"accuracy\")\\nplt.xlabel(\"epoch\")\\nplt.show()\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(hist.history[\"acc\"])\n",
    "plt.title('Model accuracy: ')\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot za trening i val skup (preciznost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history[\"acc\"])\n",
    "plt.plot(hist.history[\"val_acc\"])\n",
    "plt.title('Model accuracy: ')\n",
    "plt.legend(['Train', 'Validation'], loc='best')\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot za trening skup (greska)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history[\"loss\"])\n",
    "plt.title('Model loss: ')\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot za trening i val skup (greska)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history[\"loss\"])\n",
    "plt.plot(hist.history[\"val_loss\"])\n",
    "plt.title('Model loss: ')\n",
    "plt.legend(['Train', 'Validation'], loc='best')\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_words(seed_text, num_words, model, max_seq_len = 20):\n",
    "    for i in range(num_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen = max_seq_len, padding = 'pre')\n",
    "        \n",
    "        predicted = model.predict_classes(token_list, verbose = 0)\n",
    "        output_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "                \n",
    "        seed_text = seed_text + \" \" + output_word\n",
    "        \n",
    "    return seed_text.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Movie And A Family And A Family And Is Has And Has And Has And Has And Has And Has And Has And Has And Has And Has And Has And Has And Has And Has And Has And Has And Has And Has And Has And Has And Has And\n"
     ]
    }
   ],
   "source": [
    "print(generate_words(\"The movie\", 50, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
