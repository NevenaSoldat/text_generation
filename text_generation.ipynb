{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation\n",
    "\n",
    "The goal of this project is to demonstrate text generation using LSTM neural networks.\n",
    "Our database contains numerous movie plots taken from Wikipedia, so we will generate something similiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import LSTM, Dense, Dropout, Embedding\n",
    "from keras.models import Sequential\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "from numpy.random import seed\n",
    "\n",
    "set_random_seed(2)\n",
    "seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of plots:  34886\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"movie_plots.csv\")\n",
    "data = data[data['Plot'].isnull()==False]\n",
    "movie_plots = data['Plot']\n",
    "print(\"Number of plots: \", movie_plots.shape[0])\n",
    "movie_plots = movie_plots[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize words\n",
    "\n",
    "Generally in Natural Language Processing projects, the first step is removal of stop words, such as \"the\", \"a\", \"an\", and punctuation. We will skip this step since we want to generate human-like speech.\n",
    "Tokenization is turning unique words into unique integers. This step is necessary for preparing data for embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 80)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_words = 50000\n",
    "tokenizer = Tokenizer(num_words = max_words)\n",
    "tokenizer.fit_on_texts(movie_plots.values)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(movie_plots.values)\n",
    "sequences = pad_sequences(sequences, maxlen = 80, truncating = 'post')\n",
    "sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a single list of tokens so we can apply sliding windows\n",
    "\n",
    "text = [item for sublist in sequences for item in sublist]\n",
    "vocab_size = len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  985\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size: \", vocab_size)\n",
    "\n",
    "# reverse dictionary so we can decode tokenized sequences back to words\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data for input and output values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1580\n"
     ]
    }
   ],
   "source": [
    "seq_len = 20\n",
    "\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(len(text)-seq_len):\n",
    "    seq_in = text[i:i+seq_len]\n",
    "    seq_out = text[i+seq_len]\n",
    "    dataX.append(seq_in)\n",
    "    dataY.append(seq_out)\n",
    "    \n",
    "size = len(dataX)\n",
    "print(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1580, 20), (1580,))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataX = np.asarray(dataX)\n",
    "dataY = np.asarray(dataY)\n",
    "dataX.shape, dataY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1580, 986)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "trainX = np.reshape(dataX, (size, seq_len, 1))\n",
    "trainy = np_utils.to_categorical(dataY)\n",
    "trainy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 256)               264192    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 986)               253402    \n",
      "=================================================================\n",
      "Total params: 517,594\n",
      "Trainable params: 517,594\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import LSTM, Dense, Embedding, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "#model.add(Embedding(vocab_size+1, 50, input_length = train_len))\n",
    "model.add(LSTM(256, input_shape = (trainX.shape[1], trainX.shape[2])))\n",
    "#model.add(LSTM(100))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(trainy.shape[1], activation = 'softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1580/1580 [==============================] - 3s 2ms/step - loss: 6.4554 - acc: 0.1532\n",
      "\n",
      "Epoch 00001: loss improved from inf to 6.45543, saving model to ./weights.hdf5\n",
      "Epoch 2/20\n",
      "1580/1580 [==============================] - 3s 2ms/step - loss: 5.2525 - acc: 0.1778A: 0s - loss: 5.3047 - acc: 0.\n",
      "\n",
      "Epoch 00002: loss improved from 6.45543 to 5.25247, saving model to ./weights.hdf5\n",
      "Epoch 3/20\n",
      "1580/1580 [==============================] - 3s 2ms/step - loss: 4.9841 - acc: 0.1722\n",
      "\n",
      "Epoch 00003: loss improved from 5.25247 to 4.98410, saving model to ./weights.hdf5\n",
      "Epoch 4/20\n",
      "1580/1580 [==============================] - 3s 2ms/step - loss: 4.8581 - acc: 0.1741\n",
      "\n",
      "Epoch 00004: loss improved from 4.98410 to 4.85809, saving model to ./weights.hdf5\n",
      "Epoch 5/20\n",
      "1580/1580 [==============================] - 3s 2ms/step - loss: 4.7691 - acc: 0.1810\n",
      "\n",
      "Epoch 00005: loss improved from 4.85809 to 4.76907, saving model to ./weights.hdf5\n",
      "Epoch 6/20\n",
      "1580/1580 [==============================] - 3s 2ms/step - loss: 4.6887 - acc: 0.1766\n",
      "\n",
      "Epoch 00006: loss improved from 4.76907 to 4.68870, saving model to ./weights.hdf5\n",
      "Epoch 7/20\n",
      "1580/1580 [==============================] - 3s 2ms/step - loss: 4.6102 - acc: 0.1741\n",
      "\n",
      "Epoch 00007: loss improved from 4.68870 to 4.61021, saving model to ./weights.hdf5\n",
      "Epoch 8/20\n",
      "1580/1580 [==============================] - 3s 2ms/step - loss: 4.5338 - acc: 0.1816\n",
      "\n",
      "Epoch 00008: loss improved from 4.61021 to 4.53378, saving model to ./weights.hdf5\n",
      "Epoch 9/20\n",
      "1580/1580 [==============================] - 3s 2ms/step - loss: 4.4756 - acc: 0.1778\n",
      "\n",
      "Epoch 00009: loss improved from 4.53378 to 4.47559, saving model to ./weights.hdf5\n",
      "Epoch 10/20\n",
      "1580/1580 [==============================] - 3s 2ms/step - loss: 4.4156 - acc: 0.1753\n",
      "\n",
      "Epoch 00010: loss improved from 4.47559 to 4.41556, saving model to ./weights.hdf5\n",
      "Epoch 11/20\n",
      "1580/1580 [==============================] - 3s 2ms/step - loss: 4.3482 - acc: 0.1797\n",
      "\n",
      "Epoch 00011: loss improved from 4.41556 to 4.34818, saving model to ./weights.hdf5\n",
      "Epoch 12/20\n",
      "1580/1580 [==============================] - 3s 2ms/step - loss: 4.2845 - acc: 0.1848\n",
      "\n",
      "Epoch 00012: loss improved from 4.34818 to 4.28454, saving model to ./weights.hdf5\n",
      "Epoch 13/20\n",
      "1580/1580 [==============================] - 4s 2ms/step - loss: 4.2288 - acc: 0.1734\n",
      "\n",
      "Epoch 00013: loss improved from 4.28454 to 4.22882, saving model to ./weights.hdf5\n",
      "Epoch 14/20\n",
      "1580/1580 [==============================] - 4s 2ms/step - loss: 4.1630 - acc: 0.1854\n",
      "\n",
      "Epoch 00014: loss improved from 4.22882 to 4.16298, saving model to ./weights.hdf5\n",
      "Epoch 15/20\n",
      "1580/1580 [==============================] - 3s 2ms/step - loss: 4.1078 - acc: 0.1816\n",
      "\n",
      "Epoch 00015: loss improved from 4.16298 to 4.10776, saving model to ./weights.hdf5\n",
      "Epoch 16/20\n",
      "1580/1580 [==============================] - 3s 2ms/step - loss: 4.0470 - acc: 0.1797\n",
      "\n",
      "Epoch 00016: loss improved from 4.10776 to 4.04698, saving model to ./weights.hdf5\n",
      "Epoch 17/20\n",
      "1580/1580 [==============================] - 3s 2ms/step - loss: 3.9841 - acc: 0.1924\n",
      "\n",
      "Epoch 00017: loss improved from 4.04698 to 3.98408, saving model to ./weights.hdf5\n",
      "Epoch 18/20\n",
      "1580/1580 [==============================] - 3s 2ms/step - loss: 3.9294 - acc: 0.1861\n",
      "\n",
      "Epoch 00018: loss improved from 3.98408 to 3.92942, saving model to ./weights.hdf5\n",
      "Epoch 19/20\n",
      "1580/1580 [==============================] - 3s 2ms/step - loss: 3.8704 - acc: 0.1835\n",
      "\n",
      "Epoch 00019: loss improved from 3.92942 to 3.87041, saving model to ./weights.hdf5\n",
      "Epoch 20/20\n",
      "1580/1580 [==============================] - 4s 2ms/step - loss: 3.8064 - acc: 0.1924\n",
      "\n",
      "Epoch 00020: loss improved from 3.87041 to 3.80637, saving model to ./weights.hdf5\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "\n",
    "filepath = \"./weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor = 'loss', verbose = 1, save_best_only = True, mode = 'min')\n",
    "callbacks = [checkpoint]\n",
    "\n",
    "hist = model.fit(trainX, trainy, epochs = 20, batch_size = 128, verbose = 1, callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading weights from a checkpoint\n",
    "\n",
    "filename = \"weights.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
