{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation\n",
    "\n",
    "The goal of this project is to demonstrate text generation using LSTM neural networks.\n",
    "Our database contains numerous movie plots taken from Wikipedia, so we will generate something similiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nemanja/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/nemanja/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/nemanja/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/nemanja/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/nemanja/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/nemanja/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/nemanja/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/nemanja/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/nemanja/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/nemanja/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/nemanja/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/nemanja/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-801b9a3d56a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import LSTM, Dense, Dropout, Embedding\n",
    "from keras.models import Sequential\n",
    "import random \n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max movie plot len:  36773\n",
      "Min movie plot len:  15\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"movie_plots.csv\")\n",
    "\n",
    "movie_plots = data['Plot']\n",
    "print(\"Max movie plot len: \", movie_plots.map(len).max())\n",
    "print(\"Min movie plot len: \", movie_plots.map(len).min())\n",
    "\n",
    "all_plots = list(movie_plots.values)\n",
    "\n",
    "random.seed(42)\n",
    "sample = random.sample(all_plots, 50)\n",
    "#all_plots = all_plots[:100]\n",
    "sample[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize words\n",
    "\n",
    "Tokenization is turning unique words into unique integers. This step is necessary for preparing data for embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 102 1013   31    7    4 1377   19    7  496   20    1 2241 2242    3\n",
      "     9    7  497   21    4 1014 2243 1378   44    4  608   10  108  609\n",
      "     3  610 2244  498   39 1379   46    7  499  144   34  611  749   31\n",
      "    26    8  184 1380 2245   33  750    3   90 1381  612   14   21    8\n",
      "   500    5 2246   71  376    2  751   15    3  268   21    8  377 1378\n",
      "     1  421  749    5  752   28 1015    5 1379  501]\n",
      " [ 139  203 2436 2437   20 1073  238   22   55    3  190 2438   28  393\n",
      "  1478  341  139   88 1074    3  190   88  284 1479   74   19    7    4\n",
      "  2439  773    5    1 2440    7 2441    2    4 2442 1075  139    3  190\n",
      "    28  771    3   32    6  239   12  335  128   92  139 1480   26 2443\n",
      "    34    4 2444  147   63   47   88  284  203 2445  139   85   30  648\n",
      "     2   69    4  342    3  150    2 1481    8  140]]\n"
     ]
    }
   ],
   "source": [
    "max_words = 50000\n",
    "tokenizer = Tokenizer(num_words = max_words)\n",
    "tokenizer.fit_on_texts(sample)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(sample)\n",
    "sequences = pad_sequences(sequences, maxlen = 80, truncating = 'post')\n",
    "sequences.shape\n",
    "print(sequences[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n"
     ]
    }
   ],
   "source": [
    "# making a single list of tokens so we can apply sliding windows\n",
    "\n",
    "text = [item for sublist in sequences for item in sublist]\n",
    "print(len(text))\n",
    "vocab_size = len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  5162\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size: \", vocab_size+1)\n",
    "\n",
    "# reverse dictionary so we can decode tokenized sequences back to words\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data for input and output values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3980\n"
     ]
    }
   ],
   "source": [
    "seq_len = 20\n",
    "\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(len(text)-seq_len):\n",
    "    seq_in = text[i:i+seq_len]\n",
    "    seq_out = text[i+seq_len]\n",
    "    dataX.append(seq_in)\n",
    "    dataY.append(seq_out)\n",
    "    \n",
    "size = len(dataX)\n",
    "print(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3980, 20), (3980,))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataX = np.asarray(dataX)\n",
    "dataY = np.asarray(dataY)\n",
    "dataX.shape, dataY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3980, 5147)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "#trainX = np.reshape(dataX, (size, seq_len, 1))\n",
    "trainX = dataX\n",
    "trainy = np_utils.to_categorical(dataY)\n",
    "trainy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_15 (Embedding)     (None, 20, 10)            51620     \n",
      "_________________________________________________________________\n",
      "lstm_26 (LSTM)               (None, 20, 256)           273408    \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 20, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_27 (LSTM)               (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 5147)              1322779   \n",
      "=================================================================\n",
      "Total params: 2,173,119\n",
      "Trainable params: 2,173,119\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import LSTM, Dense, Embedding, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "#Embedding layer has 3 parameters: input_dim, output_dim, input_length\n",
    "model.add(Embedding(vocab_size+1, 10, input_length = trainX.shape[1]))\n",
    "model.add(LSTM(256,  return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "#model.add(Dropout(0.1))\n",
    "model.add(Dense(trainy.shape[1], activation = 'softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3184 samples, validate on 796 samples\n",
      "Epoch 1/50\n",
      "3184/3184 [==============================] - 43s 13ms/step - loss: 7.8018 - acc: 0.0396 - val_loss: 7.8643 - val_acc: 0.0377\n",
      "\n",
      "Epoch 00001: loss improved from inf to 7.80177, saving model to ./weights.hdf5\n",
      "Epoch 2/50\n",
      "3184/3184 [==============================] - 29s 9ms/step - loss: 6.6980 - acc: 0.0477 - val_loss: 8.0181 - val_acc: 0.0377\n",
      "\n",
      "Epoch 00002: loss improved from 7.80177 to 6.69801, saving model to ./weights.hdf5\n",
      "Epoch 3/50\n",
      "3184/3184 [==============================] - 29s 9ms/step - loss: 6.4255 - acc: 0.0452 - val_loss: 8.1055 - val_acc: 0.0377\n",
      "\n",
      "Epoch 00003: loss improved from 6.69801 to 6.42553, saving model to ./weights.hdf5\n",
      "Epoch 4/50\n",
      "3184/3184 [==============================] - 29s 9ms/step - loss: 6.3602 - acc: 0.0415 - val_loss: 8.2946 - val_acc: 0.0377\n",
      "\n",
      "Epoch 00004: loss improved from 6.42553 to 6.36017, saving model to ./weights.hdf5\n",
      "Epoch 5/50\n",
      "3184/3184 [==============================] - 28s 9ms/step - loss: 6.3386 - acc: 0.0421 - val_loss: 8.4791 - val_acc: 0.0377\n",
      "\n",
      "Epoch 00005: loss improved from 6.36017 to 6.33862, saving model to ./weights.hdf5\n",
      "Epoch 6/50\n",
      "3184/3184 [==============================] - 28s 9ms/step - loss: 6.3283 - acc: 0.0452 - val_loss: 8.5452 - val_acc: 0.0050\n",
      "\n",
      "Epoch 00006: loss improved from 6.33862 to 6.32835, saving model to ./weights.hdf5\n",
      "Epoch 7/50\n",
      "3184/3184 [==============================] - 28s 9ms/step - loss: 6.3250 - acc: 0.0446 - val_loss: 8.5490 - val_acc: 0.0377\n",
      "\n",
      "Epoch 00007: loss improved from 6.32835 to 6.32495, saving model to ./weights.hdf5\n",
      "Epoch 8/50\n",
      "3184/3184 [==============================] - 29s 9ms/step - loss: 6.3167 - acc: 0.0415 - val_loss: 8.5299 - val_acc: 0.0377\n",
      "\n",
      "Epoch 00008: loss improved from 6.32495 to 6.31672, saving model to ./weights.hdf5\n",
      "Epoch 9/50\n",
      "3184/3184 [==============================] - 28s 9ms/step - loss: 6.3143 - acc: 0.0477 - val_loss: 8.6005 - val_acc: 0.0377\n",
      "\n",
      "Epoch 00009: loss improved from 6.31672 to 6.31434, saving model to ./weights.hdf5\n",
      "Epoch 10/50\n",
      "3184/3184 [==============================] - 28s 9ms/step - loss: 6.3135 - acc: 0.0477 - val_loss: 8.7414 - val_acc: 0.0377\n",
      "\n",
      "Epoch 00010: loss improved from 6.31434 to 6.31351, saving model to ./weights.hdf5\n",
      "Epoch 11/50\n",
      "3184/3184 [==============================] - 27s 9ms/step - loss: 6.3028 - acc: 0.0471 - val_loss: 8.7738 - val_acc: 0.0377\n",
      "\n",
      "Epoch 00011: loss improved from 6.31351 to 6.30278, saving model to ./weights.hdf5\n",
      "Epoch 12/50\n",
      "3184/3184 [==============================] - 27s 9ms/step - loss: 6.2985 - acc: 0.0408 - val_loss: 8.6488 - val_acc: 0.0377\n",
      "\n",
      "Epoch 00012: loss improved from 6.30278 to 6.29849, saving model to ./weights.hdf5\n",
      "Epoch 13/50\n",
      "3184/3184 [==============================] - 28s 9ms/step - loss: 6.2935 - acc: 0.0433 - val_loss: 8.6051 - val_acc: 0.0377\n",
      "\n",
      "Epoch 00013: loss improved from 6.29849 to 6.29349, saving model to ./weights.hdf5\n",
      "Epoch 14/50\n",
      "3184/3184 [==============================] - 28s 9ms/step - loss: 6.2793 - acc: 0.0477 - val_loss: 8.7350 - val_acc: 0.0377\n",
      "\n",
      "Epoch 00014: loss improved from 6.29349 to 6.27925, saving model to ./weights.hdf5\n",
      "Epoch 15/50\n",
      "3184/3184 [==============================] - 31s 10ms/step - loss: 6.2320 - acc: 0.0437 - val_loss: 8.8443 - val_acc: 0.0126\n",
      "\n",
      "Epoch 00015: loss improved from 6.27925 to 6.23197, saving model to ./weights.hdf5\n",
      "Epoch 16/50\n",
      "3184/3184 [==============================] - 29s 9ms/step - loss: 6.1131 - acc: 0.0675 - val_loss: 8.4646 - val_acc: 0.0377\n",
      "\n",
      "Epoch 00016: loss improved from 6.23197 to 6.11306, saving model to ./weights.hdf5\n",
      "Epoch 17/50\n",
      "3184/3184 [==============================] - 28s 9ms/step - loss: 6.0233 - acc: 0.0732 - val_loss: 8.6415 - val_acc: 0.0389\n",
      "\n",
      "Epoch 00017: loss improved from 6.11306 to 6.02332, saving model to ./weights.hdf5\n",
      "Epoch 18/50\n",
      "3184/3184 [==============================] - 28s 9ms/step - loss: 5.9369 - acc: 0.0835 - val_loss: 8.7440 - val_acc: 0.0402\n",
      "\n",
      "Epoch 00018: loss improved from 6.02332 to 5.93689, saving model to ./weights.hdf5\n",
      "Epoch 19/50\n",
      "3184/3184 [==============================] - 29s 9ms/step - loss: 5.8727 - acc: 0.0832 - val_loss: 8.6250 - val_acc: 0.0427\n",
      "\n",
      "Epoch 00019: loss improved from 5.93689 to 5.87269, saving model to ./weights.hdf5\n",
      "Epoch 20/50\n",
      "3184/3184 [==============================] - 29s 9ms/step - loss: 5.8002 - acc: 0.0832 - val_loss: 9.1296 - val_acc: 0.0427\n",
      "\n",
      "Epoch 00020: loss improved from 5.87269 to 5.80018, saving model to ./weights.hdf5\n",
      "Epoch 21/50\n",
      "3184/3184 [==============================] - 28s 9ms/step - loss: 5.7253 - acc: 0.0870 - val_loss: 9.3164 - val_acc: 0.0377\n",
      "\n",
      "Epoch 00021: loss improved from 5.80018 to 5.72533, saving model to ./weights.hdf5\n",
      "Epoch 22/50\n",
      "3184/3184 [==============================] - 28s 9ms/step - loss: 5.6352 - acc: 0.0845 - val_loss: 9.4841 - val_acc: 0.0389\n",
      "\n",
      "Epoch 00022: loss improved from 5.72533 to 5.63524, saving model to ./weights.hdf5\n",
      "Epoch 23/50\n",
      "3184/3184 [==============================] - 28s 9ms/step - loss: 5.5602 - acc: 0.0920 - val_loss: 9.6547 - val_acc: 0.0389\n",
      "\n",
      "Epoch 00023: loss improved from 5.63524 to 5.56015, saving model to ./weights.hdf5\n",
      "Epoch 24/50\n",
      "3184/3184 [==============================] - 29s 9ms/step - loss: 5.4875 - acc: 0.0901 - val_loss: 9.7592 - val_acc: 0.0528\n",
      "\n",
      "Epoch 00024: loss improved from 5.56015 to 5.48747, saving model to ./weights.hdf5\n",
      "Epoch 25/50\n",
      "3184/3184 [==============================] - 33s 10ms/step - loss: 5.4242 - acc: 0.0970 - val_loss: 9.9839 - val_acc: 0.0528\n",
      "\n",
      "Epoch 00025: loss improved from 5.48747 to 5.42419, saving model to ./weights.hdf5\n",
      "Epoch 26/50\n",
      "3184/3184 [==============================] - 30s 10ms/step - loss: 5.3533 - acc: 0.0958 - val_loss: 10.1168 - val_acc: 0.0578\n",
      "\n",
      "Epoch 00026: loss improved from 5.42419 to 5.35327, saving model to ./weights.hdf5\n",
      "Epoch 27/50\n",
      "3184/3184 [==============================] - 27s 9ms/step - loss: 5.2966 - acc: 0.0961 - val_loss: 10.3879 - val_acc: 0.0691\n",
      "\n",
      "Epoch 00027: loss improved from 5.35327 to 5.29655, saving model to ./weights.hdf5\n",
      "Epoch 28/50\n",
      "3184/3184 [==============================] - 27s 9ms/step - loss: 5.2172 - acc: 0.0974 - val_loss: 10.3325 - val_acc: 0.0427\n",
      "\n",
      "Epoch 00028: loss improved from 5.29655 to 5.21724, saving model to ./weights.hdf5\n",
      "Epoch 29/50\n",
      "3184/3184 [==============================] - 28s 9ms/step - loss: 5.1466 - acc: 0.1106 - val_loss: 10.6025 - val_acc: 0.0628\n",
      "\n",
      "Epoch 00029: loss improved from 5.21724 to 5.14661, saving model to ./weights.hdf5\n",
      "Epoch 30/50\n",
      "3184/3184 [==============================] - 27s 9ms/step - loss: 5.0827 - acc: 0.1052 - val_loss: 10.7899 - val_acc: 0.0590\n",
      "\n",
      "Epoch 00030: loss improved from 5.14661 to 5.08271, saving model to ./weights.hdf5\n",
      "Epoch 31/50\n",
      "3184/3184 [==============================] - 28s 9ms/step - loss: 5.0256 - acc: 0.1087 - val_loss: 10.9393 - val_acc: 0.0553\n",
      "\n",
      "Epoch 00031: loss improved from 5.08271 to 5.02557, saving model to ./weights.hdf5\n",
      "Epoch 32/50\n",
      "3184/3184 [==============================] - 27s 8ms/step - loss: 4.9672 - acc: 0.1102 - val_loss: 10.8567 - val_acc: 0.0628\n",
      "\n",
      "Epoch 00032: loss improved from 5.02557 to 4.96722, saving model to ./weights.hdf5\n",
      "Epoch 33/50\n",
      "3184/3184 [==============================] - 28s 9ms/step - loss: 4.9006 - acc: 0.1121 - val_loss: 11.0879 - val_acc: 0.0603\n",
      "\n",
      "Epoch 00033: loss improved from 4.96722 to 4.90056, saving model to ./weights.hdf5\n",
      "Epoch 34/50\n",
      "3184/3184 [==============================] - 27s 9ms/step - loss: 4.8434 - acc: 0.1149 - val_loss: 11.0974 - val_acc: 0.0477\n",
      "\n",
      "Epoch 00034: loss improved from 4.90056 to 4.84337, saving model to ./weights.hdf5\n",
      "Epoch 35/50\n",
      "3184/3184 [==============================] - 28s 9ms/step - loss: 4.7932 - acc: 0.1128 - val_loss: 11.0224 - val_acc: 0.0490\n",
      "\n",
      "Epoch 00035: loss improved from 4.84337 to 4.79320, saving model to ./weights.hdf5\n",
      "Epoch 36/50\n",
      "3184/3184 [==============================] - 27s 9ms/step - loss: 4.7371 - acc: 0.1200 - val_loss: 11.2094 - val_acc: 0.0389\n",
      "\n",
      "Epoch 00036: loss improved from 4.79320 to 4.73707, saving model to ./weights.hdf5\n",
      "Epoch 37/50\n",
      "3184/3184 [==============================] - 27s 8ms/step - loss: 4.6841 - acc: 0.1184 - val_loss: 11.2988 - val_acc: 0.0528\n",
      "\n",
      "Epoch 00037: loss improved from 4.73707 to 4.68411, saving model to ./weights.hdf5\n",
      "Epoch 38/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3184/3184 [==============================] - 27s 9ms/step - loss: 4.6276 - acc: 0.1193 - val_loss: 11.4470 - val_acc: 0.0465\n",
      "\n",
      "Epoch 00038: loss improved from 4.68411 to 4.62764, saving model to ./weights.hdf5\n",
      "Epoch 39/50\n",
      "3184/3184 [==============================] - 27s 9ms/step - loss: 4.5749 - acc: 0.1215 - val_loss: 11.4583 - val_acc: 0.0465\n",
      "\n",
      "Epoch 00039: loss improved from 4.62764 to 4.57487, saving model to ./weights.hdf5\n",
      "Epoch 40/50\n",
      "3184/3184 [==============================] - 29s 9ms/step - loss: 4.5220 - acc: 0.1278 - val_loss: 11.4937 - val_acc: 0.0440\n",
      "\n",
      "Epoch 00040: loss improved from 4.57487 to 4.52203, saving model to ./weights.hdf5\n",
      "Epoch 41/50\n",
      "3184/3184 [==============================] - 27s 9ms/step - loss: 4.4713 - acc: 0.1300 - val_loss: 11.3906 - val_acc: 0.0440\n",
      "\n",
      "Epoch 00041: loss improved from 4.52203 to 4.47130, saving model to ./weights.hdf5\n",
      "Epoch 42/50\n",
      "3184/3184 [==============================] - 28s 9ms/step - loss: 4.4240 - acc: 0.1313 - val_loss: 11.4432 - val_acc: 0.0364\n",
      "\n",
      "Epoch 00042: loss improved from 4.47130 to 4.42405, saving model to ./weights.hdf5\n",
      "Epoch 43/50\n",
      "3184/3184 [==============================] - 28s 9ms/step - loss: 4.3754 - acc: 0.1322 - val_loss: 11.5810 - val_acc: 0.0427\n",
      "\n",
      "Epoch 00043: loss improved from 4.42405 to 4.37537, saving model to ./weights.hdf5\n",
      "Epoch 44/50\n",
      "3184/3184 [==============================] - 27s 8ms/step - loss: 4.3320 - acc: 0.1354 - val_loss: 11.6244 - val_acc: 0.0427\n",
      "\n",
      "Epoch 00044: loss improved from 4.37537 to 4.33202, saving model to ./weights.hdf5\n",
      "Epoch 45/50\n",
      "3184/3184 [==============================] - 27s 8ms/step - loss: 4.2788 - acc: 0.1338 - val_loss: 11.6335 - val_acc: 0.0465\n",
      "\n",
      "Epoch 00045: loss improved from 4.33202 to 4.27875, saving model to ./weights.hdf5\n",
      "Epoch 46/50\n",
      "3184/3184 [==============================] - 27s 8ms/step - loss: 4.2363 - acc: 0.1366 - val_loss: 11.6865 - val_acc: 0.0415\n",
      "\n",
      "Epoch 00046: loss improved from 4.27875 to 4.23630, saving model to ./weights.hdf5\n",
      "Epoch 47/50\n",
      "3184/3184 [==============================] - 27s 9ms/step - loss: 4.1868 - acc: 0.1388 - val_loss: 11.6968 - val_acc: 0.0415\n",
      "\n",
      "Epoch 00047: loss improved from 4.23630 to 4.18684, saving model to ./weights.hdf5\n",
      "Epoch 48/50\n",
      "3184/3184 [==============================] - 26s 8ms/step - loss: 4.1435 - acc: 0.1420 - val_loss: 11.7326 - val_acc: 0.0415\n",
      "\n",
      "Epoch 00048: loss improved from 4.18684 to 4.14354, saving model to ./weights.hdf5\n",
      "Epoch 49/50\n",
      "3184/3184 [==============================] - 28s 9ms/step - loss: 4.1001 - acc: 0.1486 - val_loss: 11.8052 - val_acc: 0.0415\n",
      "\n",
      "Epoch 00049: loss improved from 4.14354 to 4.10008, saving model to ./weights.hdf5\n",
      "Epoch 50/50\n",
      "3184/3184 [==============================] - 29s 9ms/step - loss: 4.0591 - acc: 0.1467 - val_loss: 11.8682 - val_acc: 0.0452\n",
      "\n",
      "Epoch 00050: loss improved from 4.10008 to 4.05908, saving model to ./weights.hdf5\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "\n",
    "filepath = \"./weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor = 'loss', verbose = 1, save_best_only = True, mode = 'min')\n",
    "callbacks = [checkpoint]\n",
    "\n",
    "hist = model.fit(trainX, trainy, epochs = 50, batch_size = 128, verbose = 1, callbacks = callbacks, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.title('Model train loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading weights from a checkpoint\n",
    "\n",
    "filename = \"weights.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_words(seed_text, num_words, model, max_seq_len = 20):\n",
    "    for i in range(num_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen = max_seq_len, padding = 'pre')\n",
    "        \n",
    "        predicted = model.predict_classes(token_list, verbose = 0)\n",
    "        output_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "                \n",
    "        seed_text = seed_text + \" \" + output_word\n",
    "        \n",
    "    return seed_text.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Man In Black The Local Village And Old\n"
     ]
    }
   ],
   "source": [
    "print(generate_words(\"man in black\", 5, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
