{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation\n",
    "\n",
    "The goal of this project is to demonstrate text generation using LSTM neural networks.\n",
    "Our database contains numerous movie plots taken from Wikipedia, so we will generate something similiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import LSTM, Dense, Dropout, Embedding\n",
    "from keras.models import Sequential\n",
    "import random \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max movie plot len:  36773\n",
      "Min movie plot len:  15\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"movie_plots.csv\")\n",
    "\n",
    "movie_plots = data['Plot']\n",
    "np.random.seed(1)\n",
    "print(\"Max movie plot len: \", movie_plots.map(len).max())\n",
    "print(\"Min movie plot len: \", movie_plots.map(len).min())\n",
    "\n",
    "all_plots = list(movie_plots.values)\n",
    "sample = random.sample(all_plots, 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize words\n",
    "\n",
    "Generally in Natural Language Processing projects, the first step is removal of stop words, such as \"the\", \"a\", \"an\", and punctuation. We will skip this step since we want to generate human-like speech.\n",
    "Tokenization is turning unique words into unique integers. This step is necessary for preparing data for embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 673    3  432 1855 1856 1857    3 1858 1859   31    4  142    3  129\n",
      "  1165 1860  433   25    1 1861    5   27  851   29 1862    4   42  238\n",
      "   432  163  852 1863 1864 1166 1865 1167 1166 1866   19  853   10  432\n",
      "   674   74   38 1168  675  432   90    2  208    1 1867    4  524   23\n",
      "   673   20   30   11  164  108  432 1169    4  434   16 1868   42  238\n",
      "    23   18    1  854 1869   38   14   27  165  268]\n",
      " [ 189   31  437 1173    3   35  122   53   31  317  165  130   20    1\n",
      "   122  239    2 1174  270   35    5   34   17    4  438  268   19   83\n",
      "   109    9   39  680   96    9 1886  271   46    4  240   12    9  142\n",
      "     3  212   18    1  122  527    2    9  142    3  528   16   80   11\n",
      "  1887    9    2    4  272   19   83   80    9  360 1888    1  272    7\n",
      "   110   35   20  143   68    6 1175   11  529   13]]\n"
     ]
    }
   ],
   "source": [
    "max_words = 50000\n",
    "tokenizer = Tokenizer(num_words = max_words)\n",
    "tokenizer.fit_on_texts(sample)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(sample)\n",
    "sequences = pad_sequences(sequences, maxlen = 80, truncating = 'post')\n",
    "sequences.shape\n",
    "print(sequences[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n"
     ]
    }
   ],
   "source": [
    "# making a single list of tokens so we can apply sliding windows\n",
    "\n",
    "text = [item for sublist in sequences for item in sublist]\n",
    "print(len(text))\n",
    "vocab_size = len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  4316\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size: \", vocab_size+1)\n",
    "\n",
    "# reverse dictionary so we can decode tokenized sequences back to words\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data for input and output values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3980\n"
     ]
    }
   ],
   "source": [
    "seq_len = 20\n",
    "\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(len(text)-seq_len):\n",
    "    seq_in = text[i:i+seq_len]\n",
    "    seq_out = text[i+seq_len]\n",
    "    dataX.append(seq_in)\n",
    "    dataY.append(seq_out)\n",
    "    \n",
    "size = len(dataX)\n",
    "print(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3980, 20), (3980,))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataX = np.asarray(dataX)\n",
    "dataY = np.asarray(dataY)\n",
    "dataX.shape, dataY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3980, 4316)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "#trainX = np.reshape(dataX, (size, seq_len, 1))\n",
    "trainX = dataX\n",
    "trainy = np_utils.to_categorical(dataY)\n",
    "trainy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 20, 10)            43160     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 256)               273408    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4316)              1109212   \n",
      "=================================================================\n",
      "Total params: 1,425,780\n",
      "Trainable params: 1,425,780\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import LSTM, Dense, Embedding, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size+1, 10, input_length = trainX.shape[1]))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(trainy.shape[1], activation = 'softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "3980/3980 [==============================] - 7s 2ms/step - loss: 7.5859 - acc: 0.0560\n",
      "\n",
      "Epoch 00001: loss improved from inf to 7.58594, saving model to ./weights.hdf5\n",
      "Epoch 2/20\n",
      "3980/3980 [==============================] - 7s 2ms/step - loss: 6.5021 - acc: 0.0555\n",
      "\n",
      "Epoch 00002: loss improved from 7.58594 to 6.50208, saving model to ./weights.hdf5\n",
      "Epoch 3/20\n",
      "3980/3980 [==============================] - 7s 2ms/step - loss: 6.3088 - acc: 0.0608\n",
      "\n",
      "Epoch 00003: loss improved from 6.50208 to 6.30876, saving model to ./weights.hdf5\n",
      "Epoch 4/20\n",
      "3980/3980 [==============================] - 8s 2ms/step - loss: 6.2545 - acc: 0.0621\n",
      "\n",
      "Epoch 00004: loss improved from 6.30876 to 6.25448, saving model to ./weights.hdf5\n",
      "Epoch 5/20\n",
      "3980/3980 [==============================] - 8s 2ms/step - loss: 6.2255 - acc: 0.0618\n",
      "\n",
      "Epoch 00005: loss improved from 6.25448 to 6.22545, saving model to ./weights.hdf5\n",
      "Epoch 6/20\n",
      "3980/3980 [==============================] - 9s 2ms/step - loss: 6.1840 - acc: 0.0613\n",
      "\n",
      "Epoch 00006: loss improved from 6.22545 to 6.18402, saving model to ./weights.hdf5\n",
      "Epoch 7/20\n",
      "3980/3980 [==============================] - 8s 2ms/step - loss: 6.1319 - acc: 0.0621\n",
      "\n",
      "Epoch 00007: loss improved from 6.18402 to 6.13193, saving model to ./weights.hdf5\n",
      "Epoch 8/20\n",
      "3980/3980 [==============================] - 9s 2ms/step - loss: 6.0764 - acc: 0.0621\n",
      "\n",
      "Epoch 00008: loss improved from 6.13193 to 6.07638, saving model to ./weights.hdf5\n",
      "Epoch 9/20\n",
      "3980/3980 [==============================] - 9s 2ms/step - loss: 6.0165 - acc: 0.0653\n",
      "\n",
      "Epoch 00009: loss improved from 6.07638 to 6.01648, saving model to ./weights.hdf5\n",
      "Epoch 10/20\n",
      "3980/3980 [==============================] - 10s 3ms/step - loss: 5.9550 - acc: 0.0688\n",
      "\n",
      "Epoch 00010: loss improved from 6.01648 to 5.95496, saving model to ./weights.hdf5\n",
      "Epoch 11/20\n",
      "3980/3980 [==============================] - 10s 2ms/step - loss: 5.8832 - acc: 0.0952\n",
      "\n",
      "Epoch 00011: loss improved from 5.95496 to 5.88322, saving model to ./weights.hdf5\n",
      "Epoch 12/20\n",
      "3980/3980 [==============================] - 10s 2ms/step - loss: 5.8229 - acc: 0.1055\n",
      "\n",
      "Epoch 00012: loss improved from 5.88322 to 5.82289, saving model to ./weights.hdf5\n",
      "Epoch 13/20\n",
      "3980/3980 [==============================] - 10s 2ms/step - loss: 5.7884 - acc: 0.1048\n",
      "\n",
      "Epoch 00013: loss improved from 5.82289 to 5.78838, saving model to ./weights.hdf5\n",
      "Epoch 14/20\n",
      "3980/3980 [==============================] - 10s 3ms/step - loss: 5.7440 - acc: 0.1060\n",
      "\n",
      "Epoch 00014: loss improved from 5.78838 to 5.74400, saving model to ./weights.hdf5\n",
      "Epoch 15/20\n",
      "3980/3980 [==============================] - 10s 3ms/step - loss: 5.7069 - acc: 0.1111\n",
      "\n",
      "Epoch 00015: loss improved from 5.74400 to 5.70692, saving model to ./weights.hdf5\n",
      "Epoch 16/20\n",
      "3980/3980 [==============================] - 11s 3ms/step - loss: 5.6616 - acc: 0.1088\n",
      "\n",
      "Epoch 00016: loss improved from 5.70692 to 5.66164, saving model to ./weights.hdf5\n",
      "Epoch 17/20\n",
      "3980/3980 [==============================] - 10s 3ms/step - loss: 5.6178 - acc: 0.1138\n",
      "\n",
      "Epoch 00017: loss improved from 5.66164 to 5.61778, saving model to ./weights.hdf5\n",
      "Epoch 18/20\n",
      "3980/3980 [==============================] - 10s 3ms/step - loss: 5.5737 - acc: 0.1126\n",
      "\n",
      "Epoch 00018: loss improved from 5.61778 to 5.57368, saving model to ./weights.hdf5\n",
      "Epoch 19/20\n",
      "3980/3980 [==============================] - 11s 3ms/step - loss: 5.5266 - acc: 0.1141\n",
      "\n",
      "Epoch 00019: loss improved from 5.57368 to 5.52662, saving model to ./weights.hdf5\n",
      "Epoch 20/20\n",
      "3980/3980 [==============================] - 13s 3ms/step - loss: 5.4763 - acc: 0.1181\n",
      "\n",
      "Epoch 00020: loss improved from 5.52662 to 5.47634, saving model to ./weights.hdf5\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "\n",
    "filepath = \"./weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor = 'loss', verbose = 1, save_best_only = True, mode = 'min')\n",
    "callbacks = [checkpoint]\n",
    "\n",
    "hist = model.fit(trainX, trainy, epochs = 20, batch_size = 128, verbose = 1, callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading weights from a checkpoint\n",
    "\n",
    "filename = \"weights.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_words(seed_text, num_words, model, max_seq_len = 20):\n",
    "    for i in range(num_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen = max_seq_len, padding = 'pre')\n",
    "        \n",
    "        predicted = model.predict_classes(token_list, verbose = 0)\n",
    "        output_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "                \n",
    "        seed_text = seed_text + \" \" + output_word\n",
    "        \n",
    "    return seed_text.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Man In The Black And A A Is And A A Is And A\n"
     ]
    }
   ],
   "source": [
    "print(generate_words(\"the man in the black\", 10, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
