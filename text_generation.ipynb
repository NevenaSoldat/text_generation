{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation\n",
    "\n",
    "The goal of this project is to demonstrate text generation using LSTM neural networks.\n",
    "Our database contains numerous movie plots taken from Wikipedia, so we will generate something similiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import LSTM, Dense, Dropout, Embedding\n",
    "from keras.models import Sequential\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "#from tensorflow import set_random_seed\n",
    "#from numpy.random import seed\n",
    "\n",
    "#set_random_seed(2)\n",
    "#seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of plots:  34886\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"movie_plots.csv\")\n",
    "data = data[data['Plot'].isnull()==False]\n",
    "movie_plots = data['Plot']\n",
    "print(\"Number of plots: \", movie_plots.shape[0])\n",
    "movie_plots = movie_plots[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize words\n",
    "\n",
    "Generally in Natural Language Processing projects, the first step is removal of stop words, such as \"the\", \"a\", \"an\", and punctuation. We will skip this step since we want to generate human-like speech.\n",
    "Tokenization is turning unique words into unique integers. This step is necessary for preparing data for embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 80)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_words = 50000\n",
    "tokenizer = Tokenizer(num_words = max_words)\n",
    "tokenizer.fit_on_texts(movie_plots.values)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(movie_plots.values)\n",
    "sequences = pad_sequences(sequences, maxlen = 80, truncating = 'post')\n",
    "sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a single list of tokens so we can apply sliding windows\n",
    "\n",
    "text = [item for sublist in sequences for item in sublist]\n",
    "vocab_size = len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  985\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size: \", vocab_size)\n",
    "\n",
    "# reverse dictionary so we can decode tokenized sequences back to words\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data for input and output values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1580\n"
     ]
    }
   ],
   "source": [
    "seq_len = 20\n",
    "\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(len(text)-seq_len):\n",
    "    seq_in = text[i:i+seq_len]\n",
    "    seq_out = text[i+seq_len]\n",
    "    dataX.append(seq_in)\n",
    "    dataY.append(seq_out)\n",
    "    \n",
    "size = len(dataX)\n",
    "print(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1580, 20), (1580,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataX = np.asarray(dataX)\n",
    "dataY = np.asarray(dataY)\n",
    "dataX.shape, dataY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1580, 986)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "trainX = np.reshape(dataX, (size, seq_len, 1))\n",
    "trainy = np_utils.to_categorical(dataY)\n",
    "trainy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 256)               264192    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 986)               253402    \n",
      "=================================================================\n",
      "Total params: 517,594\n",
      "Trainable params: 517,594\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import LSTM, Dense, Embedding, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "#model.add(Embedding(vocab_size+1, 50, input_length = train_len))\n",
    "model.add(LSTM(256, input_shape = (trainX.shape[1], trainX.shape[2])))\n",
    "#model.add(LSTM(100))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(trainy.shape[1], activation = 'softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1580/1580 [==============================] - 9s 6ms/step - loss: 6.4712 - acc: 0.1335\n",
      "\n",
      "Epoch 00001: loss improved from inf to 6.47124, saving model to ./weights.hdf5\n",
      "Epoch 2/20\n",
      "1580/1580 [==============================] - 5s 3ms/step - loss: 5.2759 - acc: 0.1747\n",
      "\n",
      "Epoch 00002: loss improved from 6.47124 to 5.27591, saving model to ./weights.hdf5\n",
      "Epoch 3/20\n",
      "1580/1580 [==============================] - 5s 3ms/step - loss: 4.9943 - acc: 0.1753\n",
      "\n",
      "Epoch 00003: loss improved from 5.27591 to 4.99426, saving model to ./weights.hdf5\n",
      "Epoch 4/20\n",
      "1580/1580 [==============================] - 5s 3ms/step - loss: 4.8629 - acc: 0.1797\n",
      "\n",
      "Epoch 00004: loss improved from 4.99426 to 4.86287, saving model to ./weights.hdf5\n",
      "Epoch 5/20\n",
      "1580/1580 [==============================] - 5s 3ms/step - loss: 4.7642 - acc: 0.1791\n",
      "\n",
      "Epoch 00005: loss improved from 4.86287 to 4.76416, saving model to ./weights.hdf5\n",
      "Epoch 6/20\n",
      "1580/1580 [==============================] - 5s 3ms/step - loss: 4.6957 - acc: 0.1734\n",
      "\n",
      "Epoch 00006: loss improved from 4.76416 to 4.69570, saving model to ./weights.hdf5\n",
      "Epoch 7/20\n",
      "1580/1580 [==============================] - 5s 3ms/step - loss: 4.6275 - acc: 0.1766\n",
      "\n",
      "Epoch 00007: loss improved from 4.69570 to 4.62751, saving model to ./weights.hdf5\n",
      "Epoch 8/20\n",
      "1580/1580 [==============================] - 5s 3ms/step - loss: 4.5627 - acc: 0.1753\n",
      "\n",
      "Epoch 00008: loss improved from 4.62751 to 4.56267, saving model to ./weights.hdf5\n",
      "Epoch 9/20\n",
      "1580/1580 [==============================] - 5s 3ms/step - loss: 4.4902 - acc: 0.1835\n",
      "\n",
      "Epoch 00009: loss improved from 4.56267 to 4.49021, saving model to ./weights.hdf5\n",
      "Epoch 10/20\n",
      "1580/1580 [==============================] - 5s 3ms/step - loss: 4.4350 - acc: 0.1816\n",
      "\n",
      "Epoch 00010: loss improved from 4.49021 to 4.43504, saving model to ./weights.hdf5\n",
      "Epoch 11/20\n",
      "1580/1580 [==============================] - 5s 3ms/step - loss: 4.3681 - acc: 0.1911\n",
      "\n",
      "Epoch 00011: loss improved from 4.43504 to 4.36805, saving model to ./weights.hdf5\n",
      "Epoch 12/20\n",
      "1580/1580 [==============================] - 5s 3ms/step - loss: 4.3080 - acc: 0.1785\n",
      "\n",
      "Epoch 00012: loss improved from 4.36805 to 4.30800, saving model to ./weights.hdf5\n",
      "Epoch 13/20\n",
      "1580/1580 [==============================] - 5s 3ms/step - loss: 4.2446 - acc: 0.1804\n",
      "\n",
      "Epoch 00013: loss improved from 4.30800 to 4.24461, saving model to ./weights.hdf5\n",
      "Epoch 14/20\n",
      "1580/1580 [==============================] - 5s 3ms/step - loss: 4.1903 - acc: 0.1804\n",
      "\n",
      "Epoch 00014: loss improved from 4.24461 to 4.19033, saving model to ./weights.hdf5\n",
      "Epoch 15/20\n",
      "1580/1580 [==============================] - 5s 3ms/step - loss: 4.1265 - acc: 0.1880\n",
      "\n",
      "Epoch 00015: loss improved from 4.19033 to 4.12648, saving model to ./weights.hdf5\n",
      "Epoch 16/20\n",
      "1580/1580 [==============================] - 5s 3ms/step - loss: 4.0669 - acc: 0.1842\n",
      "\n",
      "Epoch 00016: loss improved from 4.12648 to 4.06686, saving model to ./weights.hdf5\n",
      "Epoch 17/20\n",
      "1580/1580 [==============================] - 6s 4ms/step - loss: 4.0032 - acc: 0.1842\n",
      "\n",
      "Epoch 00017: loss improved from 4.06686 to 4.00316, saving model to ./weights.hdf5\n",
      "Epoch 18/20\n",
      "1580/1580 [==============================] - 5s 3ms/step - loss: 3.9434 - acc: 0.1886\n",
      "\n",
      "Epoch 00018: loss improved from 4.00316 to 3.94336, saving model to ./weights.hdf5\n",
      "Epoch 19/20\n",
      "1580/1580 [==============================] - 5s 3ms/step - loss: 3.8749 - acc: 0.2000\n",
      "\n",
      "Epoch 00019: loss improved from 3.94336 to 3.87492, saving model to ./weights.hdf5\n",
      "Epoch 20/20\n",
      "1580/1580 [==============================] - 6s 4ms/step - loss: 3.8126 - acc: 0.1975\n",
      "\n",
      "Epoch 00020: loss improved from 3.87492 to 3.81256, saving model to ./weights.hdf5\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "\n",
    "filepath = \"./weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor = 'loss', verbose = 1, save_best_only = True, mode = 'min')\n",
    "callbacks = [checkpoint]\n",
    "\n",
    "hist = model.fit(trainX, trainy, epochs = 20, batch_size = 128, verbose = 1, callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading weights from a checkpoint\n",
    "\n",
    "filename = \"weights.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
