{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation\n",
    "\n",
    "The goal of this project is to demonstrate text generation using LSTM neural networks.\n",
    "Our database contains numerous movie plots taken from Wikipedia, so we will generate something similiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import LSTM, Dense, Dropout, Embedding\n",
    "from keras.models import Sequential\n",
    "import random \n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import os, multiprocessing\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computer characteristics: \n",
      "RAM: 7.653069 GB\n",
      "CORES: 4\n"
     ]
    }
   ],
   "source": [
    "# Computer characteristics\n",
    "\n",
    "mem_bytes = os.sysconf('SC_PAGE_SIZE') * os.sysconf('SC_PHYS_PAGES')\n",
    "mem_gib = mem_bytes / (1024.**3)\n",
    "print(\"Computer characteristics: \")\n",
    "print(\"RAM: %f GB\" % mem_gib)\n",
    "print(\"CORES: %d\" % multiprocessing.cpu_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max movie plot len:  6752\n",
      "Min movie plot len:  2\n",
      "Min movie plot len:  300\n",
      "Max movie plot len:  3340\n",
      "5948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The story describes an encounter between a Parisian tailor named Maurice Courtelin (Chevalier) and a family of local aristocrats. These include Vicomte Gilbert de Varèze (Ruggles), who owes Maurice a large amount of money for tailoring work; Gilbert\\'s uncle the Duc d\\'Artelines (C. Aubrey Smith), the family patriarch; d\\'Artelines\\' man-hungry niece Valentine (Loy); and his other 22-year-old niece, Princesse Jeanette (MacDonald), who has been a widow for three years. D\\'Artelines has been unable to find Jeanette a new husband of suitable age and rank. The household also includes three aunts and an ineffectual suitor the Comte de Savignac (Butterworth).\\r\\nMaurice custom-tailors clothing for de Varèze on credit, but the Vicomte\\'s unpaid tailoring bills become intolerable, so Maurice travels to de Savignac\\'s castle to collect the money owed to him. On the way, he has a confrontation with Princesse Jeanette. He immediately professes his love for her, but she haughtily rejects him.\\r\\nWhen Maurice arrives at the castle, Gilbert introduces him as \"Baron Courtelin\" in order to hide the truth from the Comte . Maurice is fearful of this scheme at first, but changes his mind when he sees Jeanette. While staying at the castle, he arouses Valentine\\'s desire, charms the rest of the family except for Jeanette, saves a deer\\'s life during a hunt, and continues to woo Jeanette. The Comte de Savignac discovers that Maurice is a fake, but the Vicomte then claims that Maurice is a royal who is traveling incognito for security reasons. Finally, Jeanette succumbs to Maurice\\'s charms, telling him \"Whoever you are, whatever you are, wherever you are, I love you.\"\\r\\nWhen Maurice criticizes Jeanette\\'s tailor, the family confronts him for his rudeness, only to catch him and Jeanette alone with Jeanette partially undressed. Maurice explains that he is redesigning Jeanette\\'s riding outfit, and he proves this by successfully altering it, but in the process he is forced to reveal his true identity. Despite her earlier promise, Jeanette recoils from him and runs to her room on hearing that he is a commoner. The entire household is outraged, and Maurice leaves. However, as a train carries him back to Paris, Jeanette struggles with her fears, finally realizes her mistake, and catches up to the train on horseback. When the engineer refuses to stop the train, she rides ahead and stands on the track. The train stops, Maurice jumps out, and the two lovers embrace as steam from the train envelops them.'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"movie_plots.csv\")\n",
    "\n",
    "movie_plots = data['Plot']\n",
    "\n",
    "def count_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "print(\"Max movie plot len: \", movie_plots.map(count_words).max())\n",
    "print(\"Min movie plot len: \", movie_plots.map(count_words).min())\n",
    "\n",
    "\n",
    "# zadrzavamo samo opise sa vise od 300 reci\n",
    "movie_plots = movie_plots[movie_plots.map(count_words) > 299]\n",
    "print(\"Min movie plot len: \", movie_plots.map(count_words).min())\n",
    "# i zadrzavamo sve koji imaju manje od 500 reci\n",
    "movie_plots = movie_plots[movie_plots.map(count_words) < 501]\n",
    "print(\"Max movie plot len: \", movie_plots.map(len).max())\n",
    "\n",
    "all_plots = list(movie_plots.values)\n",
    "print(len(all_plots))\n",
    "# setting a seed so we get the same result every time\n",
    "random.seed(5)\n",
    "sample = random.sample(all_plots, 100)\n",
    "sample[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize words\n",
    "\n",
    "Tokenization is turning unique words into unique integers. This step is necessary for preparing data for embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    2  124  155   10  109  137 1895  177   26 1896    2 1897  404  710\n",
      "    6 1898    3   38  431    2  711  712   10   63    7 1899   14    1\n",
      "  124   33  423    3  650   17    3 1900    2  490  684   24 1901  362\n",
      "    1  124   33  712   44   59 1902   93  713 1903   10    6   71   26\n",
      "    1 1904   33   12    2  348  113 1905   21  438 1906   28  201    3\n",
      "  487   31   18   14  380 1907]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "max_words = 10000\n",
    "tokenizer = Tokenizer(num_words = max_words)\n",
    "tokenizer.fit_on_texts(sample)\n",
    "\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(sample)\n",
    "sequences_len = 300\n",
    "sequences = pad_sequences(sequences, maxlen = sequences_len, truncating = 'post')\n",
    "\n",
    "\n",
    "sequences.shape\n",
    "print(sequences[64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus size:  30000\n",
      "Vocabulary size:  2551\n"
     ]
    }
   ],
   "source": [
    "# making a single list of tokens so we can apply sliding windows\n",
    "\n",
    "text = [item for sublist in sequences for item in sublist]\n",
    "print(\"Corpus size: \", len(text))\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "print(\"Vocabulary size: \", vocab_size+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse dictionary so we can decode tokenized sequences back to words\n",
    "\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data for input and output values\n",
    "\n",
    "Input sequence has the size of 20 words, and output is the next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29980\n"
     ]
    }
   ],
   "source": [
    "seq_len = 20\n",
    "\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(len(text)-seq_len):\n",
    "    seq_in = text[i:i+seq_len]\n",
    "    seq_out = text[i+seq_len]\n",
    "    dataX.append(seq_in)\n",
    "    dataY.append(seq_out)\n",
    "    \n",
    "    \n",
    "lenX = len(dataX)\n",
    "print(lenX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "dataX = np.asarray(dataX)\n",
    "dataY = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15984, 20)\n",
      "(3996, 20)\n",
      "(15984, 7003)\n",
      "(3996, 7003)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "trainX, testX, trainy, testy = train_test_split(dataX, dataY, test_size = 0.2)\n",
    "print(trainX.shape)\n",
    "print(testX.shape)\n",
    "print(trainy.shape)\n",
    "print(testy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 32)            224096    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 20, 100)           53200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 20, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7003)              707303    \n",
      "=================================================================\n",
      "Total params: 1,064,999\n",
      "Trainable params: 1,064,999\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import LSTM, Dense, Embedding, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size+1, 32, input_length = trainX.shape[1]))\n",
    "model.add(LSTM(100,  return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(trainy.shape[1], activation = 'softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15984 samples, validate on 3996 samples\n",
      "Epoch 1/20\n",
      "15984/15984 [==============================] - 25s 2ms/step - loss: 6.7762 - acc: 0.2051 - val_loss: 6.5230 - val_acc: 0.2060\n",
      "\n",
      "Epoch 00001: loss improved from inf to 6.77621, saving model to ./weights.hdf5\n",
      "Epoch 2/20\n",
      "15984/15984 [==============================] - 26s 2ms/step - loss: 6.0078 - acc: 0.2065 - val_loss: 6.6309 - val_acc: 0.2060\n",
      "\n",
      "Epoch 00002: loss improved from 6.77621 to 6.00778, saving model to ./weights.hdf5\n",
      "Epoch 3/20\n",
      "15984/15984 [==============================] - 30s 2ms/step - loss: 5.9331 - acc: 0.2065 - val_loss: 6.4049 - val_acc: 0.2060ss:\n",
      "\n",
      "Epoch 00003: loss improved from 6.00778 to 5.93310, saving model to ./weights.hdf5\n",
      "Epoch 4/20\n",
      "15984/15984 [==============================] - 30s 2ms/step - loss: 5.5494 - acc: 0.2093 - val_loss: 6.1538 - val_acc: 0.2385\n",
      "\n",
      "Epoch 00004: loss improved from 5.93310 to 5.54941, saving model to ./weights.hdf5\n",
      "Epoch 5/20\n",
      "15984/15984 [==============================] - 32s 2ms/step - loss: 5.3637 - acc: 0.2364 - val_loss: 6.1617 - val_acc: 0.2490\n",
      "\n",
      "Epoch 00005: loss improved from 5.54941 to 5.36367, saving model to ./weights.hdf5\n",
      "Epoch 6/20\n",
      "15984/15984 [==============================] - 31s 2ms/step - loss: 5.2713 - acc: 0.2430 - val_loss: 6.2802 - val_acc: 0.2490\n",
      "\n",
      "Epoch 00006: loss improved from 5.36367 to 5.27126, saving model to ./weights.hdf5\n",
      "Epoch 7/20\n",
      "15984/15984 [==============================] - 32s 2ms/step - loss: 5.2129 - acc: 0.2452 - val_loss: 6.4016 - val_acc: 0.2492\n",
      "\n",
      "Epoch 00007: loss improved from 5.27126 to 5.21288, saving model to ./weights.hdf5\n",
      "Epoch 8/20\n",
      "15984/15984 [==============================] - 33s 2ms/step - loss: 5.1682 - acc: 0.2484 - val_loss: 6.4848 - val_acc: 0.2540\n",
      "\n",
      "Epoch 00008: loss improved from 5.21288 to 5.16821, saving model to ./weights.hdf5\n",
      "Epoch 9/20\n",
      "15984/15984 [==============================] - 33s 2ms/step - loss: 5.1237 - acc: 0.2524 - val_loss: 6.5336 - val_acc: 0.2548\n",
      "\n",
      "Epoch 00009: loss improved from 5.16821 to 5.12374, saving model to ./weights.hdf5\n",
      "Epoch 10/20\n",
      "15984/15984 [==============================] - 33s 2ms/step - loss: 5.0870 - acc: 0.2550 - val_loss: 6.6552 - val_acc: 0.2533\n",
      "\n",
      "Epoch 00010: loss improved from 5.12374 to 5.08701, saving model to ./weights.hdf5\n",
      "Epoch 11/20\n",
      "15984/15984 [==============================] - 34s 2ms/step - loss: 5.0479 - acc: 0.2587 - val_loss: 6.7059 - val_acc: 0.2515\n",
      "\n",
      "Epoch 00011: loss improved from 5.08701 to 5.04792, saving model to ./weights.hdf5\n",
      "Epoch 12/20\n",
      "15984/15984 [==============================] - 34s 2ms/step - loss: 5.0137 - acc: 0.2603 - val_loss: 6.8164 - val_acc: 0.2543\n",
      "\n",
      "Epoch 00012: loss improved from 5.04792 to 5.01365, saving model to ./weights.hdf5\n",
      "Epoch 13/20\n",
      "15984/15984 [==============================] - 37s 2ms/step - loss: 4.9763 - acc: 0.2611 - val_loss: 6.8305 - val_acc: 0.2477\n",
      "\n",
      "Epoch 00013: loss improved from 5.01365 to 4.97629, saving model to ./weights.hdf5\n",
      "Epoch 14/20\n",
      "15984/15984 [==============================] - 36s 2ms/step - loss: 4.9399 - acc: 0.2635 - val_loss: 6.9067 - val_acc: 0.2523\n",
      "\n",
      "Epoch 00014: loss improved from 4.97629 to 4.93994, saving model to ./weights.hdf5\n",
      "Epoch 15/20\n",
      "15984/15984 [==============================] - 36s 2ms/step - loss: 4.9053 - acc: 0.2649 - val_loss: 6.9710 - val_acc: 0.2485\n",
      "\n",
      "Epoch 00015: loss improved from 4.93994 to 4.90529, saving model to ./weights.hdf5\n",
      "Epoch 16/20\n",
      "15984/15984 [==============================] - 35s 2ms/step - loss: 4.8694 - acc: 0.2666 - val_loss: 6.9749 - val_acc: 0.2480\n",
      "\n",
      "Epoch 00016: loss improved from 4.90529 to 4.86945, saving model to ./weights.hdf5\n",
      "Epoch 17/20\n",
      "15984/15984 [==============================] - 38s 2ms/step - loss: 4.8333 - acc: 0.2687 - val_loss: 7.0305 - val_acc: 0.2457\n",
      "\n",
      "Epoch 00017: loss improved from 4.86945 to 4.83327, saving model to ./weights.hdf5\n",
      "Epoch 18/20\n",
      "15984/15984 [==============================] - 37s 2ms/step - loss: 4.7959 - acc: 0.2695 - val_loss: 7.0540 - val_acc: 0.2485\n",
      "\n",
      "Epoch 00018: loss improved from 4.83327 to 4.79586, saving model to ./weights.hdf5\n",
      "Epoch 19/20\n",
      "15984/15984 [==============================] - 38s 2ms/step - loss: 4.7577 - acc: 0.2711 - val_loss: 7.1766 - val_acc: 0.2515\n",
      "\n",
      "Epoch 00019: loss improved from 4.79586 to 4.75771, saving model to ./weights.hdf5\n",
      "Epoch 20/20\n",
      "15984/15984 [==============================] - 36s 2ms/step - loss: 4.7203 - acc: 0.2730 - val_loss: 7.1142 - val_acc: 0.2442\n",
      "\n",
      "Epoch 00020: loss improved from 4.75771 to 4.72027, saving model to ./weights.hdf5\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "\n",
    "filepath = \"./weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor = 'loss', \n",
    "                             verbose = 1, save_best_only = True, mode = 'min')\n",
    "callbacks = [checkpoint]\n",
    "\n",
    "hist = model.fit(trainX, trainy, epochs = 20, batch_size = 128, \n",
    "                 verbose = 1, callbacks = callbacks, validation_split = 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"20e100n.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "3996/3996 [==============================] - 2s 439us/step\n",
      "Loss: 5.11\n",
      "Accuracy: 25.98 %\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on test set (accuracy and error)\n",
    "\n",
    "print(model.metrics_names)\n",
    "results = model.evaluate(testX, testy, batch_size = 128)\n",
    "print('Loss: %.2f'% results[0])\n",
    "print('Accuracy: %.2f'%(results[1]*100), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot za trening skup (preciznost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nplt.plot(hist.history[\"acc\"])\\nplt.plot(hist.history[\"val_acc\"])\\nplt.title(\\'Model accuracy: \\')\\nplt.legend([\\'Train\\', \\'Validation\\'], loc=\\'upper right\\')\\nplt.ylabel(\"accuracy\")\\nplt.xlabel(\"epoch\")\\nplt.show()\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(hist.history[\"acc\"])\n",
    "plt.title('Model accuracy: ')\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot za trening i val skup (preciznost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history[\"acc\"])\n",
    "plt.plot(hist.history[\"val_acc\"])\n",
    "plt.title('Model accuracy: ')\n",
    "plt.legend(['Train', 'Validation'], loc='best')\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot za trening skup (greska)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history[\"loss\"])\n",
    "plt.title('Model loss: ')\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot za trening i val skup (greska)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history[\"loss\"])\n",
    "plt.plot(hist.history[\"val_loss\"])\n",
    "plt.title('Model loss: ')\n",
    "plt.legend(['Train', 'Validation'], loc='best')\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_words(seed_text, num_words, model, max_seq_len = 20):\n",
    "    for i in range(num_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen = max_seq_len, padding = 'pre')\n",
    "        \n",
    "        predicted = model.predict_classes(token_list, verbose = 0)\n",
    "        output_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "                \n",
    "        seed_text = seed_text + \" \" + output_word\n",
    "        \n",
    "    return seed_text.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Movie And A Family And A Family And Is Has And Has And Has And Has And Has And Has And Has And Has And Has And Has And Has And Has And Has And Has And Has And Has And Has And Has And Has And Has And Has And\n"
     ]
    }
   ],
   "source": [
    "print(generate_words(\"The movie\", 50, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
