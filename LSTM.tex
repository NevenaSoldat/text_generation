% !TEX encoding = UTF-8 Unicode
\documentclass[a4paper]{article}

\usepackage{color}
\usepackage{url}
\usepackage[T2A]{fontenc} % enable Cyrillic fonts
\usepackage[utf8]{inputenc} % make weird characters work
\usepackage{graphicx}

\usepackage[table]{xcolor}


\usepackage[english,serbian]{babel}
%\usepackage[english,serbianc]{babel} %ukljuciti babel sa ovim opcijama, umesto gornjim, ukoliko se koristi cirilica

\usepackage[unicode]{hyperref}
\hypersetup{colorlinks,citecolor=blue,filecolor=green,linkcolor=blue,urlcolor=blue}

\usepackage{listings}

%\newtheorem{primer}{Пример}[section] %ćirilični primer
\newtheorem{primer}{Primer}[section]

\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}

\lstset{ 
  backgroundcolor=\color{white},   % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}; should come as last argument
  basicstyle=\scriptsize\ttfamily,        % the size of the fonts that are used for the code
  breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
  breaklines=true,                 % sets automatic line breaking
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{mygreen},    % comment style
  deletekeywords={...},            % if you want to delete keywords from the given language
  escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
  extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  firstnumber=1000,                % start line enumeration with line 1000
  frame=single,	                   % adds a frame around the code
  keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
  keywordstyle=\color{blue},       % keyword style
  language=Python,                 % the language of the code
  morekeywords={*,...},            % if you want to add more keywords to the set
  numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                   % how far the line-numbers are from the code
  numberstyle=\tiny\color{mygray}, % the style that is used for the line-numbers
  rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,          % underline spaces within strings only
  showtabs=false,                  % show tabs within strings adding particular underscores
  stepnumber=2,                    % the step between two line-numbers. If it's 1, each line will be numbered
  stringstyle=\color{mymauve},     % string literal style
  tabsize=2,	                   % sets default tabsize to 2 spaces
  title=\lstname                   % show the filename of files included with \lstinputlisting; also try caption instead of title
}

\begin{document}

\title{Razvoj LSTM neuronske mreže i primena nad problemom sekvencijalnog učenja\\ \small{Seminarski rad u okviru kursa\\Računarska inteligencija\\ Matematički fakultet}}

\author{Milena Kurtić, Nevena Soldat\\ mimikurtic67@gmail.com, nevenasoldat@gmail.com}

%\date{14.~april 2015.}

\maketitle

\abstract{
U okviru ovog rada predstavljen je primer implementacije LSTM neuronske mreže u programskom jeziku Python, uz korišćenje biblioteka za rad kao što su pandas, numpy, keras i druge. Prvi deo rada opisuje teorijsku osnovu iza ovih mreža. Zatim je opisana primena na problem generisanja teksta. 
Videćemo koje su njene prednosti, kao i ograničenja. Pokazana je razlika u ponašanju modela sa različitim konfiguracijama. }

\tableofcontents

\newpage

\section{Uvod}
\label{sec:uvod}

Neuronske mreže (eng. neural networks) predstavljaju najpopularniju i jednu od najprimenjenijih metoda mašinskog učenja. Njihove primene su mnogobrojne i pomeraju domete veštačke inteligencije, računarstva i primenjene matematike. Postoji više vrsta neuronskih mreža: potpuno povezane, konvolutivne, rekurentne, grafovske neuronske mreže. U vrstu rekurentnih spada i LSTM neuronska mreza.

Osnovna ideja veštačke neuronske mreže je simulacija velike količine gusto napakovanih, međusobno  povezanih nervnih  ćelija u okviru računara, tako da je omogućeno učenje pojmova, prepoznavanje šablona i donošenje odluka na način koji je sličan čovekovom. Suštinski, veštačke neuronske mreže su softverske simulacije, napravljene programirajući obične računare koji rade u uobičajenom režimu sa svojim tranzistorima i serijski povezanim logičkim  kolima, tako da se ponašaju kao da su napravljene od milijardu međusobno povezanih ćelija mozga koje rade paralelno. 

\subsection{Opis rada}

U ovom radu je implementirana rekurentna neuronska mreža koja automatski generiše određeni tekst. Podaci koje koristimo predstavljaju kratke opise filmova. Cilj nam je da za određenu sekvencu reči uspemo da pretpostavimo koja će biti sledeća. Korišćena je posebna verzija rekurentne mreže koja ume da pamti duge sekvence - kratka dugoročna memorija (eng. Long Short Term Memory - LSTM). 

Mnogi radovi bave se automatskim generisanjem teksta. Rad koji je sličan našem opisuje automatsko generisanje kratkih priča na nivou generisanja pojedinačnih reči  \cite{pawade2018story}. Automatsko generisanje teksta može biti implementirano i na nivou karaktera, što je opisano u radu \cite{sutskever2011generating}.

LSTM mreže koriste se za razne probleme nad sekvencijalnim podacima. Tako imamo radove koji se bave automatskim generisanjem reči pesama \cite{potash2015ghostwriter}, ili čak automatskim komponovanjem muzike \cite{choi2016text}.

\section{Rekurentne neuronske mreže}
Rekurentne neuronske mreže (eng. Recurrent Neural Networks - RNN) predstavljaju arhitekturu mreža specijalizovanu za obradu sekvencijalnih podataka, poput rečenica prirodnog jezika i vremenskih serija. Sekvence nameću važnost redosleda zapažanja podataka, kako prilikom treniranja modela, tako i prilikom predviđanja. Mreže su konstruisane sa idejom da se modeluje zavisnost među instancama. Elementi ulazne sekvence se obrađuju u koracima, mreža ima skriveno stanje koje akumulira informaciju o elementima sekvence obrađenim u prethodnim koracima, a parametri određuju na koji način se to stanje menja iz koraka u korak na osnovu prethodnog stanja i tekućih ulaza i kako se generiše izlaz u zavisnosti od tekućeg stanja.

"Learning of sequential data continues to be a fundamental task and a challenge in
pattern recognition and machine learning. Applications involving sequential data
may require prediction of new events, generation of new sequences, or decision
making such as classification of sequences or sub-sequences."

     — On Prediction Using Variable Order Markov Models, 2004.

\subsection{Arhitektura RNN}

Rekurentne neuronske mreže sadrže petlje koje obezbeđuju čuvanje informacija \cite{understandingLSTM}.

\begin{figure}[htp]
    \centering
    \includegraphics[scale=0.4]{RNN-rolled.png}
    \caption{Prikaz RNN mreže}
    \label{fig:rnn}
\end{figure}

Na slici \ref{fig:rnn} prikazan je jedan deo neuronske mreže, A, čiji je ulaz $x_t$, a izlaz $h_t$. Petlja omogućava da se informacije prosleđuju iz jednog koraka u mreži u drugi. Biće jednostavnije ako ih zamislimo na sledeći način:

\begin{figure}[htp]
    \centering
    \includegraphics[scale=0.4]{RNN-unrolled.png}
    \caption{Prikaz RNN mreže u obliku lanca}
    \label{fig:rnn2}
\end{figure}

Rekurentna mreža predstavlja veći broj istih mreža, koje prenose poruke narednom delu u lancu, što je prikazano na slici \ref{fig:rnn2}. 

\subsection{Problemi sa RNN}
Postoje dva osnovna problema rekurentnih neuronskih mreža u njihovoj osnovnoj formi. Prvi se tiče problema nestajućih i eksplodirajućih gradijenata. Naime, graf izračunavanja rekurentne neuronske mreže je tipično vrlo dubok zbog velike dužine sekvence. Usled toga, prilikom izračunavanja gradijenta propagacijom u prošlost, dolazi do velikog broja množenja koja neretko čine da koordinate gradijenta ili eksplodiraju ili nestanu.

Drugi problem se odnosi na dugoročno čuvanje informacije i modelovanje dugoročnih zavisnosti u podacima. Kako se skriveno stanje u svakom koraku dobija linearnom kombinacijom prethodnog stanja i ulaza, doprinos starijih ulaza se brzo gubi pod uticajem novih. Dugoročno čuvanje relevantnih informacija nije moguće.

Oba ova problema se prevazilaze upotrebom duge kratkoročne memorije (eng. long short term memory), skraćeno LSTM, što je složena jedinica mreže sa specifičnom strukturom koja omogućava kontrolu čitanja i upisa u jedinicu. Upravo ova jedinica dovela je do ključnih uspeha rekurentnih neuronskih mreža i predstavlja standardni izbor prilikom formulisanja modela rekurentne mreže. \cite{matf}


\subsection{LSTM neuronske mreže}

LSTM neuronske mreže su specijalizovane za prevazilaženje problema kratkotrajne memorije. Pamćenje dugih sekvenci je praktično njihovo podrazumevano ponašanje, a ne nešto sa čime se muče. Kao što je već prikazano, sve rekurentne mreže imaju lančanu formu, sastavljenu od modula mreže koji se ponavljaju. Kod LSTM mreža, struktura ovih modula je malo drugačija. Umesto jednog sloja mreže, imamo četiri koja su povezana na veoma poseban način (Slika \ref{fig:lstm3}).

\begin{figure}[htp]
    \centering
    \includegraphics[scale=0.3]{LSTM3-chain.png}
    \caption{Prikaz lanca LSTM mreže}
    \label{fig:lstm3}
\end{figure}

Na slici iznad, svaka linija prenosi vektor podataka, od izlaza iz prethodnog čvora, do ulaza u naredni. Roze krugovi predstavljaju operacije, poput sabiranja vektora, dok žuti pravougaonici predstavljaju slojeve mreže. Linije koje se spajaju obeležavaju spajanje informacija, dok linije koje se razdvajaju predstavljaju sadržaj koji se kopira, gde kopije idu na različita mesta (Slika \ref{fig:lstm4}).

\begin{figure}[htp]
    \centering
    \includegraphics[scale=0.3]{LSTM2-notation.png}
    \caption{Notacija}
    \label{fig:lstm4}
\end{figure}

Osnovna ideja LSTM-a je postojanje takozvane ćelije koja čuva skriveno stanje, uz kontrolu pisanja, čitanja i zaboravljanja. Na slici \ref{fig:lstm5} je to horizontalna linija na vrhu dijagrama.

\begin{figure}[htp]
    \centering
    \includegraphics[scale=0.4]{LSTM3-C-line.png}
    \caption{Ćelija LSTM mreže}
    \label{fig:lstm5}
\end{figure}


Stanje ćelije prolazi kroz ceo lanac, uz male linearne interakcije. Informacije se veoma lako prenose nepromenjene. LSTM može da doda ili ukloni informacije iz stanja ćelije, pomoću struktura koje se nazivaju kapije. Kapije odlučuju koje informacije i u kojoj količini će biti očuvane. 

Prvi korak u našoj LSTM mreži je odabir informacija koje će biti obrisane iz stanja ćelije. Kapija koja kontroliše ovu operaciju naziva se ,,kapija zaboravljanja``. Ovaj sloj koristi sigmoidnu aktivacionu funkciju, čiji je izlaz interval brojeva između 0 i 1. Vrednost 1 znači da se potpuno zadržava sadržaj, a 0 da se potpuno briše (Slika \ref{fig:lstm6}).


\begin{figure}[htp]
    \centering
    \includegraphics[scale=0.4]{LSTM3-focus-f.png}
    \caption{Kapija zaboravljanja}
    \label{fig:lstm6}
\end{figure}

Sledeći korak predstavlja odlučivanje o tome koje nove informacije ćemo dodati u stanje ćelije. Sastoji se iz dva dela. Prvo, sloj koji predstavlja ,,ulaznu kapiju`` odlučuje koje će vrednosti ažurirati. Zatim naredni sloj, čija je aktivaciona funkcija tangens hiperbolički, pravi vektor novih vrednosti ($\tilde{C}_t$), koje se mogu dodati u stanje ćelije. U sledećem koraku kombinujemo ova dva dela i pravimo novo stanje ćelije (Slika \ref{fig:lstm7}).

\begin{figure}[htp]
    \centering
    \includegraphics[scale=0.4]{LSTM3-focus-i.png}
    \caption{Ulazna kapija}
    \label{fig:lstm7}
\end{figure}


Sada prethodno stanje ćelije $C_{t-1}$ ažuriramo u novo $C_t$.  Pomnožimo staro stanje sa $f_t$, time zaboravljajući ono što smo odlučili da zaboravimo. Zatim dodamo $i_t \times \tilde{C}_t$. Ovo su potencijalne nove vrednosti, skalirane za onoliko koliko smo odlučili da ažuriramo svaku vrednost stanja (Slika \ref{fig:lstm8}). 

\begin{figure}[htp]
    \centering
    \includegraphics[scale=0.4]{LSTM3-focus-C.png}
    \caption{Računanje novog stanja ćelije}
    \label{fig:lstm8}
\end{figure}


Na kraju je potrebno da odlučimo šta će biti izlaz. Ova vrednost zavisi od stanja ćelije, uz male promene. Prvo prolazimo kroz sloj sa sigmoidnom aktivacionom funkcijom koji odlučuje koje delove ćelije ćemo staviti u izlaz. Zatim prođemo kroz aktivacionu funkciju tangens hiperbolički (kako bismo uokvirili vrednosti u interval između -1 i 1), i pomnožimo sa izlazom iz sloja sa sigmoidnom funkcijom kako bismo u izlaz stavili samo ono što smo odlučili (Slika \ref{fig:lstm9}).

\begin{figure}[htp]
    \centering
    \includegraphics[scale=0.4]{LSTM3-focus-o.png}
    \caption{Računanje izlaza}
    \label{fig:lstm9}
\end{figure}



\section{Problem generisanja teksta}

Generisanje prirodnog jezika (eng. Natural Language Generation - NLG) se intenzivno proučava zahvaljujući bitnim primenama koje ima, kao što su automatsko generisanje dijaloga, automatsko prevođenje jezika, sumarizacija teksta, generisanje opisa fotografija, i mnogih drugih. Poboljšanje kvaliteta automatskog generisanja teksta (u smislu sintakse i semantike) dolazi sa primenom tehnika mašinskog učenja \cite{wang2018automatic}. .

Ovaj rad bavi se automatskim generisanjem teksta na nivou reči. Podaci koje koristimo sadrže kratke opise filmova, preuzete sa vikipedije (mogu se preuzeti sa adrese: \url{https://www.kaggle.com/jrobischon/wikipedia-movie-plots}). Cilj je da se dobije kratak opis neke radnje filma, sa zadatim ulaznim tekstom. Ne očekujemo da rezultati budu precizni, bitno je da predviđeni tekst bude čitljiv.

Generisanje teksta vrši se tako da za datu početnu sekvencu teksta mreža predviđa sledeću reč, a zatim na osnovu novodobijene reči ponovo predviđa novu reč sve dok ne izgeneriše traženi broj reči. Faze rešavanja problema su sledeće:
\begin{itemize}
    \item Preprocesiranje ulaznih podataka
    \item Kreiranje i treniranje modela
    \item Rezultati
\end{itemize}


\subsection{Preprocesiranje ulaznih podataka}

Ulazni podaci su preuzeti iz baze podataka koja sadrži radnje filmova. Na slučajan način biramo 100 filmova. Različiti filmovi imaju različite i radnje, pa je potrebno da model prvo razume reči i kontekst svake radnje.

Prvi korak u svim projektima koji podrazumevaju korišćenje reči prirodnog jezika je uklanjanje stop \footnote{ovo su uglavnom veznici, kao i reči koje su veoma učestale i koje iz tog razloga ometaju istraživanje} reči. Ipak, nama je cilj generisanje teksta koji će ličiti na povezanu priču, te nam je potrebno da sačuvamo sve reči. Naredni koraci u obradi teksta podrazumevaju:
\begin{itemize}
    \item konvertovanje svih slova u male
    \item uklanjanje znakova interpunkcije, kao i ostalih netipičnih karaktera
    \item uklanjanje belina
    \item ...
\end{itemize}

Dodatno, neuronske mreže ne mogu da rade sa rečima, te tekstualne podatke kodiramo kao brojeve koji će predstavljati ulaz, odnosno izlaz iz mreže. Tokenizer API iz biblioteke keras obavlja ovaj posao za nas. Naime, pozivom funkcije fit\_on\_texts obrađuje se tekst na gorenaveden način, i kreira se rečnik čiji su ključevi reči, a vrednosti brojevi koji su im dodeljeni.

\subsubsection{Način predstavljanja teksta}

Najjednostavniji način predstavljanja reči u metodama mašinskog učenja je vektor sa jednom jedinicom, tzv. 1-hot vektor. Ovim pristupom dobijaju se vektori velike dužine, a to u realnim primenama gde je vokabular koji se koristiri prilično velikih razmera nije optimalno. Drugi problem ovog pristupa je što algoritam nema nikakvo znanje o eventualnoj sličnosti između reči. 

Pristup koji koristimo podrazumeva reprezentaciju reči u latentnom prostoru (eng. word embedding). Ovim pristupom vrši se redukcija dimenzionalnosti 1-hot reprezentacije u vektorski prostor čije koordinate nemaju direktno jasnog smisla čoveku (stoga i naziv latentni vektorski prostor). Dimenzionalnost tog vektorskog prostora zavisi od dalje primene, ali kreće se od nekoliko desetina do nekoliko hiljada.  Ključna razlika u odnosu na 1-hot reprezentaciju je što je ona gusta (eng. dense), dok je prethodna retka (eng. sparse). Retke reprezentacije imaju svojih prednosti kada je u pitanju vremenska složenost nekih algoritama, prvenstveno onih u kojima ima operacija sa retkim vektorima i matricama. S obzirom na prirodu i arhitekturu neuronskih mreža, ova prednost ne postoji - neuronskim mrežama pogodna je gusta predstava, odnosno gusti vektori osobina reči.

Biblioteka Keras nam nudi Embedding sloj koji se može koristiti u ove svrhe. Sloj se inicijalizuje nasumičnim težinama, a zatim uči vektore svih reči pri treniranju skupa podataka. Sastoji se od tri argumenta: 
\begin{itemize}
    \item input\_dim - veličina rečnika koji se koristi 
    \item output\_dim - dimenzija vektorskog prostora u kom su reči reprezentovane. Definiše dužinu vektora koji predstavljaju izlaz iz sloja za svaku reč. Može biti na primer 32, 100, ili čak više. Testiranjem treba proveriti šta najviše odgovara.
    \item input\_length - dužina ulazne sekvence. Broj reči po dokumentu
\end{itemize}

Kako svi filmovi imaju opise različite dužine, radimo poravnanje. Uzimamo 200 reči iz svakog opisa. Sve opise spajamo u jedan korpus i na osnovu njega formiramo ulazne i izlazne podatke. Ulazni su sekvence dužine 20 reči, izlaz je naredna reč.

Na slici \ref{fig:text_seq} prikazan je primer kreiranja ulaza i izlaza, gde je ulaz sekvenca dužine četiri reči, a izlaz je peta reč. Ulaz je uokviren plavom linijom, a izlaz je obojen u narandžasto.


\begin{figure}[htp]
    \centering
    \includegraphics[scale=0.2]{text_seq.png}
    \caption{Primer kreiranja ulaza i izlaza}
    \label{fig:text_seq}
\end{figure}



\subsection{Kreiranje i treniranje modela}

Koristićemo dva LSTM sloja u mreži. Dodatni skriveni slojevi produbljuju mrežu, čime ovaj model više zaslužuje naziv tehnike dubokog učenja. Dubina mreže se generalno povezuje sa njihovim uspehom, pri primeni na razne probleme predviđanja. 

"The success of deep neural networks is commonly attributed to the hierarchy that is introduced due to the several layers. Each layer processes some part of the task we wish to solve, and passes it on to the next. In this sense, the DNN can be seen as a processing pipeline, in which each layer solves a part of the task before passing it on to the next, until finally the last layer provides the output."

— Training and Analyzing Deep Recurrent Neural Networks, 2013  \cite{NIPS2013_5166}\\
\\
U okviru rada ,,Speech Recognition with Deep Recurrent Neural Networks`` \cite{graves2013speech} zaključeno je da je dubina neuronske mreže bitnija od broja neurona u okviru datog sloja.
\\
\\
Mreža sadrži:

\begin{enumerate}
    \item \textbf{Ulazni sloj}. Koristimo Embedding sloj radi pogodne reprezentacije reči. 
    \item \textbf{Prvi LSTM sloj}. Sadrži nekoliko stotina skrivenih neurona. O uticaju broja neurona na uspešnost mreže biće reči kasnije.
    \item \textbf{Sloj izbacivanja}. Radi sprečavanja pojave preprilagođavanja. Konvergencija je sporija, ali i kvalitetnija. Generalizacija je bolja zahvaljujući tome što se u treniranju izbegava to da jedan neuron bude sam odgovoran za neke primere, s obzirom na to da prisustvo neurona nije garantovano.
    \item \textbf{Drugi LSTM sloj}. Isti kao prvi.
    \item \textbf{Izlazni sloj}. Koristi softmax funkciju aktivacije, koja dodeljuje verovatnoću svakom mogućem izlazu. 
\end{enumerate}


\section{Rezultati}

U ovom poglavlju analiziraćemo uspešnost rešavanja problema predikcije sledećih N reči u rečenici. Opisane su uočene ralike u ponasanju modela sa promenom broja neurona i broja epoha. 

\subsection{Brzina konvergencije}
% TODO: GRAFIK TACNOST-EPOHE iz hist se iscrta

Slika \ref{fig:konv_20} prikazuje brzinu konvergencije prilikom treniranja modela koji ima po 100 neurona u svakom LSTM sloju, dužine trajanja 20 epoha.

\begin{figure}[htp]
    \centering
    \includegraphics[scale = 0.3]{konv_20e100n.png}
    \caption{Grafik konvergencije u 20 epoha}
    \label{fig:konv_20}
\end{figure}

Na slici \ref{fig:konv_50} je prikazana brzina konvergencije tokom 50 epoha.

\begin{figure}[htp]
    \centering
    \includegraphics[scale = 0.3]{konv_50e100n.png}
    \caption{Grafik konvergencije u 50 epoha}
    \label{fig:konv_50}
\end{figure}


Na slici \ref{fig:konv_100} je prikazana brzina konvergencije tokom 100 epoha.

\begin{figure}[!htp]
    \centering
    \includegraphics[scale = 0.3]{konv_100e100n.png}
    \caption{Grafik konvergencije u 100 epoha}
    \label{fig:konv_100}
\end{figure}


\subsection{Veličina mreže i broj epoha}

Jedno od pitanja koje nema definitivan odgovor kada god se radi o neuronskim mrežama je veličina mreže. Drugim rečima, pitanje je koliko skrivenih neurona staviti kako bi se izbalansiralo između dobrih performansi na trening delu podataka i dovoljno dobre generalizacije. Suština ove dileme leži u balansiranju između potprilagođavanja i preprilagođavanja. Naime, ukoliko skrivenim slojevima dodelimo previše neurona, tada će model veoma dobro naučiti podatke trening skupa, ali će varijansa biti prevelika. S druge strane, previše jednostavan model nije u stanju da nauči sve zakonitosti koje vladaju u skupu koji mu je dodeljen.

Broj epoha je parametar koji definiše koliko će puta algoritam učenja raditi sa čitavim skupom trening podataka. Jedna epoha znači da je svaki uzorak u skupu podataka za obuku imao mogućnost ažuriranja internih parametara modela. Sa porastom broja epoha smanjuje se greška modela. 

Probaćemo nekoliko različitih veličina mreža i broja epoha i naći onu optimalnu.

\begin{table}[!h]
    \centering
    \begin{tabular}{|| c c c c ||}
    \hline
    Broj epoha & Broj neurona & Tacnost & Gubitak \\
    \hline\hline
     20 & 100 & 27.38 & 5.11 \\
     50 & 100 & 38.18 & 4.27 \\
     100 & 100 & 43.88 & 3.88 \\
     20 & 200 & 27.80 & 4.83 \\
     50 & 200 & 38.78 & 3.96 \\
     \hline
    \end{tabular}
    \caption{Varijacije modela}
    \label{tab:tabela1}
\end{table}


Tabela \ref{tab:tabela1}  potvrđuje da porastom broja neurona i broja epoha dobijamo bolji model. Vidimo da najveću tačnost ima model sa 100 epoha i 100 neurona kao i najmanji gubitak u odnosu na ostale testirane modele.
Ovim pokazujemo da nas ne vodi samo promena broja neurona ka najboljem modelu. Potrebno je istraživati različite parametre kako bi došli do najboljeg, odnosno, onog koji će davati najsmislenije rezultate.

Rezultat za model koji ima 100 epoha i 100 neurona prikazan je slikom ispod  \ref{fig:Rezultat100e100n}. Ulazni tekst je 'The movie' i vraćeno je 50 narednih reči koje generiše naš model.

\begin{figure}[htp]
    \centering
    \includegraphics[scale=0.3]{tekst_100e100n.png}
    \caption{Rezultat 100e100n}
    \label{fig:Rezultat100e100n}
\end{figure}

Model vraća smislene reči za dati ulaz. U nastavku prikazane su slike za ostale varijacije modela kako bi čitalac imao više osećaja o razlikama. Notacija 100e100n označava da je reč o modelu koji ima 100 neurona u LSTM sloju i da je treniran kroz 100 epoha.

\begin{figure}[!ht]
    \centering
    \includegraphics[scale=0.3]{tekst_20e100n.png}
    \caption{Rezultat 20e100n}
    \label{fig:Rezultat20e100n}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[scale=0.3]{tekst_50e100n.png}
    \caption{Rezultat 50e100n}
    \label{fig:Rezultat50e100n}
\end{figure}
\begin{figure}[!ht]
    \centering
    \includegraphics[scale=0.3]{tekst_20e200n.png}
    \caption{Rezultat 20e200n}
    \label{fig:Rezultat20e200n}
\end{figure}
\begin{figure}[!ht]
    \centering
    \includegraphics[scale=0.3]{tekst_50e200n.png}
    \caption{Rezultat 50e200n}
    \label{fig:Rezultat50e200n}
\end{figure}

\section{Zaključak}

Zahvaljujući tome što modeluju prelaze iz stanja u stanje, umesto direktne zavisnosti izlaza od celokupnog ulaza, rekurentne mreže su u stanju da predstave vrlo duboke strukture zavisnosti i da obrađuju sekvence promenljive, a velike dužine. Zahvaljujući LSTM jedinicama, takođe su u stanju da ublaže problem nestajućih gradijenata i da modeluju dugoročne zavisnosti. 

Ipak, zbog kompleksnosti podataka koje modeluju i broja parametara koje LSTM jedinice imaju, nije ih lako obučavati (npr. jedna od poteškoća je haotično ponašanje u slučaju velikih apsolutnih vrednosti parametara). Poznato je da je njihovo obučavanje teže hardverski ubrzati zbog visokog obima memorijskog protoka i sekvencijalne prirode izračunavanja (dok hardversko ubrzanje počiva na paralelizaciji), što je ključ efikasnog obučavanja neuronskih mreža.

\subsection{Dalji razvoj}

Svakako, prikazano rešenje nije najbolje. Postoje razni načini kojima se LSTM neuronska mreža moze poboljšati za ovaj problem a neki od saveta za dalji rad dati su u nastavku.  
\begin{itemize}
    \item Povećajte broj epoha na više stotina, ili čak stotine stotina.
    \item Eksperimentišite sa broj neurona kao i brojem slojeva mreže.
    \item Razmisliti o slojevima napustanja, odnosno, zaboravljanja i procentima za iste.
    \item Menjati parametre, uočiti kako utiču na rezultat i jedni na druge.
\end{itemize}

\label{sec:zakljucak}




\addcontentsline{toc}{section}{Literatura}
\appendix
\bibliography{bibliografija.bib} 
\bibliographystyle{plain}

\end{document}
