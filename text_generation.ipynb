{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation\n",
    "\n",
    "The goal of this project is to demonstrate text generation using LSTM neural networks.\n",
    "Our database contains numerous movie plots taken from Wikipedia, so we will generate something similiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import LSTM, Dense, Dropout, Embedding\n",
    "from keras.models import Sequential\n",
    "import random \n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import os, multiprocessing\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computer characteristics: \n",
      "RAM: 7.653069 GB\n",
      "CORES: 4\n"
     ]
    }
   ],
   "source": [
    "# Computer characteristics\n",
    "\n",
    "mem_bytes = os.sysconf('SC_PAGE_SIZE') * os.sysconf('SC_PHYS_PAGES')\n",
    "mem_gib = mem_bytes / (1024.**3)\n",
    "print(\"Computer characteristics: \")\n",
    "print(\"RAM: %f GB\" % mem_gib)\n",
    "print(\"CORES: %d\" % multiprocessing.cpu_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max movie plot len:  36773\n",
      "Min movie plot len:  15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'United States Air Force Colonel William Hughes (Paul Kelly) asks Major Paul Peterson (John Payne), who has been called back to active service, to join a team at the Air Research and Development Command conducting tests on a downward ejection seat for bombardiers in the new Boeing B-47 Stratojet bomber. The first tests used articulated dummies, but human test subjects are needed. Besides Colonel Hughes, German scientist Dr. Franz Gruener (Gregory Gaye), also is in charge of the test program, working directly with the test subjects. Captain Jack Nolan (Richard Crane) is also assigned to the project.\\r\\nThe first volunteer, Captain Mike Cavallero (Eddie Firestone), suffers a broken neck when his parachute opens too early. He survives the test but is hospitalized. The next subject is Lieutenant Edward Simmons, to be followed by Paul. When Mike is suddenly rushed to hospital with an appendicitis attack, Paul moves up. Worried because he has a wife and son, Paul is reluctant to go, but then finds out that Captain Nolan has been killed in a B-47 crash, and as the bombardier, he might not have been able to escape the aircraft.\\r\\nHis wife (Karen Steele) begs his commanding officer to release Paul from his commitment. When Paul shows up to take the test, he finds Colonel Hughes suiting up. Imploring him to reconsider, Paul makes the case for doing the test to prove that a bailout is possible from the high-speed jet bomber. Flying with Dr. Gruener, Paul ejects, but when the ground observers ask him to indicate he is well by spread-eagling, he does not respond. On board the rescue launch, they pick up Paul and find he is fine; he was simply concentrating so hard that he forgot to spread-eagle. After he is cleared by the medics, Paul is greeted by Carol and his son Kit (Richard Eyer) and, with their blessing, decides to continue with the project.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"movie_plots.csv\")\n",
    "\n",
    "movie_plots = data['Plot']\n",
    "print(\"Max movie plot len: \", movie_plots.map(len).max())\n",
    "print(\"Min movie plot len: \", movie_plots.map(len).min())\n",
    "\n",
    "all_plots = list(movie_plots.values)\n",
    "# setting a seed so we get the same result every time\n",
    "random.seed(42)\n",
    "sample = random.sample(all_plots, 100)\n",
    "sample[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize words\n",
    "\n",
    "Tokenization is turning unique words into unique integers. This step is necessary for preparing data for embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   9 3264 3265  815  472    7  104  182  815   75   25 1188    2  969\n",
      "   348 2079    2  530 3266   60  429  815    3 2080  104  116 2081 2082\n",
      "     5 2083 1513    3    4 2084  599  429    3  116 1189  182 3267   44\n",
      "   699    1  473 1190   16    1 1514    5   10  531   10 3268  816    2\n",
      "   182  238   10    3   10 1515  532 3269   53  815 3270 3271   14 1516\n",
      "   116  128   26    1  430 2085   11  182    3  533   10   30   32    1\n",
      "  1513   42    8 3272  211  815   52 2086  429  317  248    2 3273    1\n",
      "    52  970   19    1  348 2080  104  116    3  104    9  600  971  474\n",
      "   117   63   55  161   59    2  212 3274    9  318    2 2087    1 1517\n",
      "  2088  382 2089   79  183    2  601   59 3275  182    2  532    9  318\n",
      "     2 3276    1 3277 2090 1518 1517 2088    9    1  969  348   61  532\n",
      "    63  972    2  162    4 1519   20   38   43 1191    2  142   62    3\n",
      "    63   55  383    2  817    1   52  475   15 1192    7 2091   53 2092\n",
      "    30    7   88  700   42    7 3278  117  971   79  319    3  163   12\n",
      "     8   79  249   12]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "max_words = 10000\n",
    "tokenizer = Tokenizer(num_words = max_words)\n",
    "tokenizer.fit_on_texts(sample)\n",
    "\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(sample)\n",
    "sequences_len = 200\n",
    "sequences = pad_sequences(sequences, maxlen = sequences_len, truncating = 'post')\n",
    "\n",
    "\n",
    "sequences.shape\n",
    "print(sequences[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus size:  20000\n",
      "Vocabulary size:  7003\n"
     ]
    }
   ],
   "source": [
    "# making a single list of tokens so we can apply sliding windows\n",
    "\n",
    "text = [item for sublist in sequences for item in sublist]\n",
    "print(\"Corpus size: \", len(text))\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "print(\"Vocabulary size: \", vocab_size+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse dictionary so we can decode tokenized sequences back to words\n",
    "\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data for input and output values\n",
    "\n",
    "Input sequence has the size of 20 words, and output is the next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19980\n"
     ]
    }
   ],
   "source": [
    "seq_len = 20\n",
    "\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(len(text)-seq_len):\n",
    "    seq_in = text[i:i+seq_len]\n",
    "    seq_out = text[i+seq_len]\n",
    "    dataX.append(seq_in)\n",
    "    dataY.append(seq_out)\n",
    "    \n",
    "    \n",
    "lenX = len(dataX)\n",
    "print(lenX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "dataX = np.asarray(dataX)\n",
    "dataY = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15984, 20)\n",
      "(3996, 20)\n",
      "(15984, 7003)\n",
      "(3996, 7003)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "trainX, testX, trainy, testy = train_test_split(dataX, dataY, test_size = 0.2)\n",
    "print(trainX.shape)\n",
    "print(testX.shape)\n",
    "print(trainy.shape)\n",
    "print(testy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 32)            224096    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 20, 100)           53200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 20, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7003)              707303    \n",
      "=================================================================\n",
      "Total params: 1,064,999\n",
      "Trainable params: 1,064,999\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import LSTM, Dense, Embedding, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size+1, 32, input_length = trainX.shape[1]))\n",
    "model.add(LSTM(100,  return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(trainy.shape[1], activation = 'softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15984 samples, validate on 3996 samples\n",
      "Epoch 1/20\n",
      "15984/15984 [==============================] - 25s 2ms/step - loss: 6.7762 - acc: 0.2051 - val_loss: 6.5230 - val_acc: 0.2060\n",
      "\n",
      "Epoch 00001: loss improved from inf to 6.77621, saving model to ./weights.hdf5\n",
      "Epoch 2/20\n",
      "15984/15984 [==============================] - 26s 2ms/step - loss: 6.0078 - acc: 0.2065 - val_loss: 6.6309 - val_acc: 0.2060\n",
      "\n",
      "Epoch 00002: loss improved from 6.77621 to 6.00778, saving model to ./weights.hdf5\n",
      "Epoch 3/20\n",
      "15984/15984 [==============================] - 30s 2ms/step - loss: 5.9331 - acc: 0.2065 - val_loss: 6.4049 - val_acc: 0.2060ss:\n",
      "\n",
      "Epoch 00003: loss improved from 6.00778 to 5.93310, saving model to ./weights.hdf5\n",
      "Epoch 4/20\n",
      "15984/15984 [==============================] - 30s 2ms/step - loss: 5.5494 - acc: 0.2093 - val_loss: 6.1538 - val_acc: 0.2385\n",
      "\n",
      "Epoch 00004: loss improved from 5.93310 to 5.54941, saving model to ./weights.hdf5\n",
      "Epoch 5/20\n",
      "15984/15984 [==============================] - 32s 2ms/step - loss: 5.3637 - acc: 0.2364 - val_loss: 6.1617 - val_acc: 0.2490\n",
      "\n",
      "Epoch 00005: loss improved from 5.54941 to 5.36367, saving model to ./weights.hdf5\n",
      "Epoch 6/20\n",
      "15984/15984 [==============================] - 31s 2ms/step - loss: 5.2713 - acc: 0.2430 - val_loss: 6.2802 - val_acc: 0.2490\n",
      "\n",
      "Epoch 00006: loss improved from 5.36367 to 5.27126, saving model to ./weights.hdf5\n",
      "Epoch 7/20\n",
      "15984/15984 [==============================] - 32s 2ms/step - loss: 5.2129 - acc: 0.2452 - val_loss: 6.4016 - val_acc: 0.2492\n",
      "\n",
      "Epoch 00007: loss improved from 5.27126 to 5.21288, saving model to ./weights.hdf5\n",
      "Epoch 8/20\n",
      "15984/15984 [==============================] - 33s 2ms/step - loss: 5.1682 - acc: 0.2484 - val_loss: 6.4848 - val_acc: 0.2540\n",
      "\n",
      "Epoch 00008: loss improved from 5.21288 to 5.16821, saving model to ./weights.hdf5\n",
      "Epoch 9/20\n",
      "15984/15984 [==============================] - 33s 2ms/step - loss: 5.1237 - acc: 0.2524 - val_loss: 6.5336 - val_acc: 0.2548\n",
      "\n",
      "Epoch 00009: loss improved from 5.16821 to 5.12374, saving model to ./weights.hdf5\n",
      "Epoch 10/20\n",
      "15984/15984 [==============================] - 33s 2ms/step - loss: 5.0870 - acc: 0.2550 - val_loss: 6.6552 - val_acc: 0.2533\n",
      "\n",
      "Epoch 00010: loss improved from 5.12374 to 5.08701, saving model to ./weights.hdf5\n",
      "Epoch 11/20\n",
      "15984/15984 [==============================] - 34s 2ms/step - loss: 5.0479 - acc: 0.2587 - val_loss: 6.7059 - val_acc: 0.2515\n",
      "\n",
      "Epoch 00011: loss improved from 5.08701 to 5.04792, saving model to ./weights.hdf5\n",
      "Epoch 12/20\n",
      "15984/15984 [==============================] - 34s 2ms/step - loss: 5.0137 - acc: 0.2603 - val_loss: 6.8164 - val_acc: 0.2543\n",
      "\n",
      "Epoch 00012: loss improved from 5.04792 to 5.01365, saving model to ./weights.hdf5\n",
      "Epoch 13/20\n",
      "15984/15984 [==============================] - 37s 2ms/step - loss: 4.9763 - acc: 0.2611 - val_loss: 6.8305 - val_acc: 0.2477\n",
      "\n",
      "Epoch 00013: loss improved from 5.01365 to 4.97629, saving model to ./weights.hdf5\n",
      "Epoch 14/20\n",
      "15984/15984 [==============================] - 36s 2ms/step - loss: 4.9399 - acc: 0.2635 - val_loss: 6.9067 - val_acc: 0.2523\n",
      "\n",
      "Epoch 00014: loss improved from 4.97629 to 4.93994, saving model to ./weights.hdf5\n",
      "Epoch 15/20\n",
      "15984/15984 [==============================] - 36s 2ms/step - loss: 4.9053 - acc: 0.2649 - val_loss: 6.9710 - val_acc: 0.2485\n",
      "\n",
      "Epoch 00015: loss improved from 4.93994 to 4.90529, saving model to ./weights.hdf5\n",
      "Epoch 16/20\n",
      "15984/15984 [==============================] - 35s 2ms/step - loss: 4.8694 - acc: 0.2666 - val_loss: 6.9749 - val_acc: 0.2480\n",
      "\n",
      "Epoch 00016: loss improved from 4.90529 to 4.86945, saving model to ./weights.hdf5\n",
      "Epoch 17/20\n",
      "15984/15984 [==============================] - 38s 2ms/step - loss: 4.8333 - acc: 0.2687 - val_loss: 7.0305 - val_acc: 0.2457\n",
      "\n",
      "Epoch 00017: loss improved from 4.86945 to 4.83327, saving model to ./weights.hdf5\n",
      "Epoch 18/20\n",
      "15984/15984 [==============================] - 37s 2ms/step - loss: 4.7959 - acc: 0.2695 - val_loss: 7.0540 - val_acc: 0.2485\n",
      "\n",
      "Epoch 00018: loss improved from 4.83327 to 4.79586, saving model to ./weights.hdf5\n",
      "Epoch 19/20\n",
      "15984/15984 [==============================] - 38s 2ms/step - loss: 4.7577 - acc: 0.2711 - val_loss: 7.1766 - val_acc: 0.2515\n",
      "\n",
      "Epoch 00019: loss improved from 4.79586 to 4.75771, saving model to ./weights.hdf5\n",
      "Epoch 20/20\n",
      "15984/15984 [==============================] - 36s 2ms/step - loss: 4.7203 - acc: 0.2730 - val_loss: 7.1142 - val_acc: 0.2442\n",
      "\n",
      "Epoch 00020: loss improved from 4.75771 to 4.72027, saving model to ./weights.hdf5\n"
     ]
    }
   ],
   "source": [
    "# Nemoj da pokreces ovo\n",
    "\n",
    "\"\"\"\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "\n",
    "filepath = \"./weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor = 'loss', \n",
    "                             verbose = 1, save_best_only = True, mode = 'min')\n",
    "callbacks = [checkpoint]\n",
    "\n",
    "hist = model.fit(trainX, trainy, epochs = 20, batch_size = 128, \n",
    "                 verbose = 1, callbacks = callbacks, validation_split=0.2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ovde ucitavamo tezine koje smo vec dobile treniranjem modela\n",
    "filename = \"20e100n.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "3996/3996 [==============================] - 2s 515us/step\n",
      "Train loss: 5.11\n",
      "Train accuracy: 25.98 %\n"
     ]
    }
   ],
   "source": [
    "# Ove evaluiramo dobijene modele na test podacima\n",
    "\n",
    "print(model.metrics_names)\n",
    "results = model.evaluate(testX, testy, batch_size = 128)\n",
    "print('Train loss: %.2f'% results[0])\n",
    "print('Train accuracy: %.2f'%(results[1]*100), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nplt.plot(hist.history[\"acc\"])\\nplt.plot(hist.history[\"val_acc\"])\\nplt.title(\\'Model accuracy: \\')\\nplt.legend([\\'Train\\', \\'Validation\\'], loc=\\'upper right\\')\\nplt.ylabel(\"accuracy\")\\nplt.xlabel(\"epoch\")\\nplt.show()\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "plt.plot(hist.history[\"acc\"])\n",
    "plt.plot(hist.history[\"val_acc\"])\n",
    "plt.title('Model accuracy: ')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_words(seed_text, num_words, model, max_seq_len = 20):\n",
    "    for i in range(num_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen = max_seq_len, padding = 'pre')\n",
    "        \n",
    "        predicted = model.predict_classes(token_list, verbose = 0)\n",
    "        output_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "                \n",
    "        seed_text = seed_text + \" \" + output_word\n",
    "        \n",
    "    return seed_text.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_words(\"The movie\", 50, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
