{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation\n",
    "\n",
    "The goal of this project is to demonstrate text generation using LSTM neural networks.\n",
    "Our database contains numerous movie plots taken from Wikipedia, so we will generate something similiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import LSTM, Dense, Dropout, Embedding\n",
    "from keras.models import Sequential\n",
    "import random \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max movie plot len:  36773\n",
      "Min movie plot len:  15\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"movie_plots.csv\")\n",
    "\n",
    "movie_plots = data['Plot']\n",
    "text = movie_plots.str.lower()\n",
    "np.random.seed(1)\n",
    "print(\"Max movie plot len: \", movie_plots.map(len).max())\n",
    "print(\"Min movie plot len: \", movie_plots.map(len).min())\n",
    "\n",
    "all_plots = list(movie_plots.values)\n",
    "sample = random.sample(all_plots, 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize words\n",
    "\n",
    "Generally in Natural Language Processing projects, the first step is removal of stop words, such as \"the\", \"a\", \"an\", and punctuation. We will skip this step since we want to generate human-like speech.\n",
    "Tokenization is turning unique words into unique integers. This step is necessary for preparing data for embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 673    3  432 1855 1856 1857    3 1858 1859   31    4  142    3  129\n",
      "  1165 1860  433   25    1 1861    5   27  851   29 1862    4   42  238\n",
      "   432  163  852 1863 1864 1166 1865 1167 1166 1866   19  853   10  432\n",
      "   674   74   38 1168  675  432   90    2  208    1 1867    4  524   23\n",
      "   673   20   30   11  164  108  432 1169    4  434   16 1868   42  238\n",
      "    23   18    1  854 1869   38   14   27  165  268]\n",
      " [ 189   31  437 1173    3   35  122   53   31  317  165  130   20    1\n",
      "   122  239    2 1174  270   35    5   34   17    4  438  268   19   83\n",
      "   109    9   39  680   96    9 1886  271   46    4  240   12    9  142\n",
      "     3  212   18    1  122  527    2    9  142    3  528   16   80   11\n",
      "  1887    9    2    4  272   19   83   80    9  360 1888    1  272    7\n",
      "   110   35   20  143   68    6 1175   11  529   13]]\n"
     ]
    }
   ],
   "source": [
    "max_words = 50000\n",
    "tokenizer = Tokenizer(num_words = max_words)\n",
    "tokenizer.fit_on_texts(sample)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(sample)\n",
    "sequences = pad_sequences(sequences, maxlen = 80, truncating = 'post')\n",
    "sequences.shape\n",
    "print(sequences[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n"
     ]
    }
   ],
   "source": [
    "# making a single list of tokens so we can apply sliding windows\n",
    "\n",
    "text = [item for sublist in sequences for item in sublist]\n",
    "print(len(text))\n",
    "vocab_size = len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  4316\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size: \", vocab_size+1)\n",
    "\n",
    "# reverse dictionary so we can decode tokenized sequences back to words\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data for input and output values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3980\n"
     ]
    }
   ],
   "source": [
    "seq_len = 20\n",
    "\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(len(text)-seq_len):\n",
    "    seq_in = text[i:i+seq_len]\n",
    "    seq_out = text[i+seq_len]\n",
    "    dataX.append(seq_in)\n",
    "    dataY.append(seq_out)\n",
    "    \n",
    "size = len(dataX)\n",
    "print(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3980, 20), (3980,))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataX = np.asarray(dataX)\n",
    "dataY = np.asarray(dataY)\n",
    "dataX.shape, dataY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3980, 4316)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "#trainX = np.reshape(dataX, (size, seq_len, 1))\n",
    "trainX = dataX\n",
    "trainy = np_utils.to_categorical(dataY)\n",
    "trainy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-134b26053853>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m#model.add(LSTM(100))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import LSTM, Dense, Embedding, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size+1, 10, input_length = trainX.shape[1]))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(trainy.shape[1], activation = 'softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/nemanja/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/20\n",
      "1580/1580 [==============================] - 3s 2ms/step - loss: 6.5144 - acc: 0.1405\n",
      "\n",
      "Epoch 00001: loss improved from inf to 6.51435, saving model to ./weights.hdf5\n",
      "Epoch 2/20\n",
      "1580/1580 [==============================] - 3s 2ms/step - loss: 5.2748 - acc: 0.1715\n",
      "\n",
      "Epoch 00002: loss improved from 6.51435 to 5.27483, saving model to ./weights.hdf5\n",
      "Epoch 3/20\n",
      "1580/1580 [==============================] - 3s 2ms/step - loss: 4.9907 - acc: 0.1772\n",
      "\n",
      "Epoch 00003: loss improved from 5.27483 to 4.99067, saving model to ./weights.hdf5\n",
      "Epoch 4/20\n",
      "1580/1580 [==============================] - 3s 2ms/step - loss: 4.8434 - acc: 0.1766\n",
      "\n",
      "Epoch 00004: loss improved from 4.99067 to 4.84339, saving model to ./weights.hdf5\n",
      "Epoch 5/20\n",
      "1580/1580 [==============================] - 3s 2ms/step - loss: 4.7526 - acc: 0.1747\n",
      "\n",
      "Epoch 00005: loss improved from 4.84339 to 4.75256, saving model to ./weights.hdf5\n",
      "Epoch 6/20\n",
      "1580/1580 [==============================] - 3s 2ms/step - loss: 4.6842 - acc: 0.1747\n",
      "\n",
      "Epoch 00006: loss improved from 4.75256 to 4.68417, saving model to ./weights.hdf5\n",
      "Epoch 7/20\n",
      "1580/1580 [==============================] - 3s 2ms/step - loss: 4.6236 - acc: 0.1785\n",
      "\n",
      "Epoch 00007: loss improved from 4.68417 to 4.62357, saving model to ./weights.hdf5\n",
      "Epoch 8/20\n",
      "1580/1580 [==============================] - 3s 2ms/step - loss: 4.5488 - acc: 0.1778\n",
      "\n",
      "Epoch 00008: loss improved from 4.62357 to 4.54877, saving model to ./weights.hdf5\n",
      "Epoch 9/20\n",
      "1580/1580 [==============================] - 4s 2ms/step - loss: 4.4777 - acc: 0.1816\n",
      "\n",
      "Epoch 00009: loss improved from 4.54877 to 4.47770, saving model to ./weights.hdf5\n",
      "Epoch 10/20\n",
      "1580/1580 [==============================] - 3s 2ms/step - loss: 4.4044 - acc: 0.1848\n",
      "\n",
      "Epoch 00010: loss improved from 4.47770 to 4.40436, saving model to ./weights.hdf5\n",
      "Epoch 11/20\n",
      "1580/1580 [==============================] - 3s 2ms/step - loss: 4.3349 - acc: 0.1816\n",
      "\n",
      "Epoch 00011: loss improved from 4.40436 to 4.33495, saving model to ./weights.hdf5\n",
      "Epoch 12/20\n",
      "1580/1580 [==============================] - 3s 2ms/step - loss: 4.2766 - acc: 0.1842\n",
      "\n",
      "Epoch 00012: loss improved from 4.33495 to 4.27663, saving model to ./weights.hdf5\n",
      "Epoch 13/20\n",
      "1580/1580 [==============================] - 4s 2ms/step - loss: 4.2133 - acc: 0.1823\n",
      "\n",
      "Epoch 00013: loss improved from 4.27663 to 4.21333, saving model to ./weights.hdf5\n",
      "Epoch 14/20\n",
      "1580/1580 [==============================] - 3s 2ms/step - loss: 4.1388 - acc: 0.1810\n",
      "\n",
      "Epoch 00014: loss improved from 4.21333 to 4.13884, saving model to ./weights.hdf5\n",
      "Epoch 15/20\n",
      "1580/1580 [==============================] - 3s 2ms/step - loss: 4.0677 - acc: 0.1880\n",
      "\n",
      "Epoch 00015: loss improved from 4.13884 to 4.06773, saving model to ./weights.hdf5\n",
      "Epoch 16/20\n",
      "1580/1580 [==============================] - 3s 2ms/step - loss: 4.0077 - acc: 0.1886\n",
      "\n",
      "Epoch 00016: loss improved from 4.06773 to 4.00774, saving model to ./weights.hdf5\n",
      "Epoch 17/20\n",
      "1580/1580 [==============================] - 3s 2ms/step - loss: 3.9394 - acc: 0.1911\n",
      "\n",
      "Epoch 00017: loss improved from 4.00774 to 3.93936, saving model to ./weights.hdf5\n",
      "Epoch 18/20\n",
      "1580/1580 [==============================] - 3s 2ms/step - loss: 3.8693 - acc: 0.1975\n",
      "\n",
      "Epoch 00018: loss improved from 3.93936 to 3.86935, saving model to ./weights.hdf5\n",
      "Epoch 19/20\n",
      "1580/1580 [==============================] - 3s 2ms/step - loss: 3.8002 - acc: 0.2127\n",
      "\n",
      "Epoch 00019: loss improved from 3.86935 to 3.80024, saving model to ./weights.hdf5\n",
      "Epoch 20/20\n",
      "1580/1580 [==============================] - 3s 2ms/step - loss: 3.7298 - acc: 0.2127\n",
      "\n",
      "Epoch 00020: loss improved from 3.80024 to 3.72976, saving model to ./weights.hdf5\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "\n",
    "filepath = \"./weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor = 'loss', verbose = 1, save_best_only = True, mode = 'min')\n",
    "callbacks = [checkpoint]\n",
    "\n",
    "hist = model.fit(trainX, trainy, epochs = 20, batch_size = 128, verbose = 1, callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading weights from a checkpoint\n",
    "\n",
    "filename = \"weights.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_words(seq, max_len = 20):\n",
    "    seed = tokenizer.texts_to_sequences([seq])\n",
    "    print(seed)\n",
    "    while len(seed[0]) < max_len:\n",
    "        seed2 = pad_sequences(seed[-19:], maxlen = 19)\n",
    "        op = model.predict(np.asarray(seed2).reshape(1,-1))\n",
    "        seed[0].append(op.argmax() + 1)\n",
    "        \n",
    "    return \" \".join(map(lambda x: reverse_word_map[x], seed[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
